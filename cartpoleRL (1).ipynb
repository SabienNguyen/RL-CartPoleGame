{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWd0akYn8EGL",
        "outputId": "c22d2a45-56e5-471c-d1b1-cef7a26be450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CyXnr3k8XCP",
        "outputId": "81aa9eba-db2b-4a44-ae59-a977c39ef1d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NraH1Sag8dxe",
        "outputId": "03750762-5ba8-4ef5-f3d3-15ebe2f1e3d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04089038, -0.0216548 , -0.04991237, -0.01148612], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install pyvirtualdisplay==0.2.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT34__A-9Pa5",
        "outputId": "3b626bec-c9f0-4849-dba2-aa5badbf3420"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-utils x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 12 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 8,045 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 8,045 kB in 1s (7,013 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../01-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../02-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../03-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../04-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../05-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Collecting EasyProcess (from pyvirtualdisplay==0.2.*)\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-1.1 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ],
      "metadata": {
        "id": "yu7-RbtW9d8u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "before_training = \"before_training.mp4\"\n",
        "video = VideoRecorder(env, before_training)\n",
        "env.reset()\n",
        "\n",
        "for i in range(200):\n",
        "  env.render()\n",
        "  video.capture_frame()\n",
        "  # env.action_space.sample()\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "  # print(\"step\", i, observation, reward, done, info)\n",
        "video.close()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSCaJP7q8h_3",
        "outputId": "4a1ffcc3-cd87-44a3-af9d-5dd7b25c2ad5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from base64 import b64encode\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "  f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZCVCWdN-lUG",
        "outputId": "84b4de3c-67b4-416d-8d14-27f922e87229"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "html = render_mp4(before_training)\n",
        "HTML(html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "cf5EFqLtAVWk",
        "outputId": "42493bc8-6be4-4fd2-eae0-d7b1d928af1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'ray[rllib]==2.0.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FIHeH7J1AcZy",
        "outputId": "eed2c011-b983-4551-a0a9-f5e632442add"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[rllib]==2.0.0\n",
            "  Downloading ray-2.0.0-cp310-cp310-manylinux2014_x86_64.whl (59.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (23.1.0)\n",
            "Collecting click<=8.0.4,>=7.0 (from ray[rllib]==2.0.0)\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.0.7)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (2.31.0)\n",
            "Collecting virtualenv (from ray[rllib]==2.0.0)\n",
            "  Downloading virtualenv-20.24.6-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.43.0,>=1.42.0 (from ray[rllib]==2.0.0)\n",
            "  Downloading grpcio-1.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.5.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (0.9.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[rllib]==2.0.0)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (0.1.8)\n",
            "Collecting gym<0.24.0,>=0.21.0 (from ray[rllib]==2.0.0)\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4 (from ray[rllib]==2.0.0)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.0.0) (1.11.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from grpcio<=1.43.0,>=1.42.0->ray[rllib]==2.0.0) (1.16.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.0.0) (2.8.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]==2.0.0) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]==2.0.0) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]==2.0.0) (0.12.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]==2.0.0) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]==2.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]==2.0.0) (2023.7.22)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.0.0) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.0.0) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.0.0) (1.4.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv->ray[rllib]==2.0.0)\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv->ray[rllib]==2.0.0) (3.11.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701347 sha256=101302ed96bd401eb87df4bf8da39d6e4946565b401a9784bf41175a062a7a28\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/00/fb/fe5cf2860fb9b7bc860e28f00095a1f42c7b726dd6f42d1acc\n",
            "Successfully built gym\n",
            "Installing collected packages: distlib, virtualenv, tensorboardX, lz4, gym, grpcio, click, ray\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.2\n",
            "    Uninstalling grpcio-1.59.2:\n",
            "      Successfully uninstalled grpcio-1.59.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "google-cloud-bigquery 3.12.0 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires grpcio<2.0.0dev,>=1.44.0, but you have grpcio 1.43.0 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.43.0 which is incompatible.\n",
            "tensorboard 2.14.1 requires grpcio>=1.48.2, but you have grpcio 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.0.4 distlib-0.3.7 grpcio-1.43.0 gym-0.23.1 lz4-4.3.2 ray-2.0.0 tensorboardX-2.6.2.2 virtualenv-20.24.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.shutdown()\n",
        "ray.init(\n",
        "    num_cpus=3,\n",
        "    include_dashboard=False,\n",
        "    ignore_reinit_error=True,\n",
        "    log_to_driver=False,\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"env\": \"CartPole-v0\",\n",
        "    # Change the following line to `\"framework\": \"tf\"` to use TensorFlow\n",
        "    \"framework\": \"torch\",\n",
        "    \"model\": {\n",
        "        \"fcnet_hiddens\": [32],\n",
        "        \"fcnet_activation\": \"linear\"  # Removed the extra comma here\n",
        "    },\n",
        "}\n",
        "\n",
        "stop = {\"episode_reward_mean\": 195}\n",
        "\n",
        "# Execute training\n",
        "analysis = ray.tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop=stop,\n",
        "    checkpoint_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ZGH3qVAEzow",
        "outputId": "b85221d2-fede-4b06-ade0-eb1909395024"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n",
            "/usr/local/lib/python3.10/dist-packages/flax/configurations.py:42: DeprecationWarning: jax.config.define_bool_state is deprecated. Please use other libraries for configuration instead.\n",
            "  return jax_config.define_bool_state('flax_' + name, default, help)\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n",
            "2023-11-15 23:29:06,218\tINFO worker.py:1518 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2023-11-15 23:34:45 (running for 00:05:36.52)<br>Memory usage on this node: 3.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/3 CPUs, 0/1 GPUs, 0.0/6.94 GiB heap, 0.0/3.47 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c676f_00000</td><td>TERMINATED</td><td>172.28.0.12:1698</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         313.722</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">   196.2</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ray/util/placement_group.py:80: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.\n",
            "  ).remote(self)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/ray/actor.py:637: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.\n",
            "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/actor.py:637: DeprecationWarning: placement_group_bundle_index parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.\n",
            "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/actor.py:637: DeprecationWarning: placement_group_capture_child_tasks parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.\n",
            "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 4000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_agent_steps_trained: 4000\n",
            "    num_env_steps_sampled: 4000\n",
            "    num_env_steps_trained: 4000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-29-41\n",
            "  done: false\n",
            "  episode_len_mean: 20.88421052631579\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 84.0\n",
            "  episode_reward_mean: 20.88421052631579\n",
            "  episode_reward_min: 8.0\n",
            "  episodes_this_iter: 190\n",
            "  episodes_total: 190\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6832666425935684\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.9779622337830964\n",
            "          kl: 0.009415175194389674\n",
            "          policy_loss: -0.017341304719147663\n",
            "          total_loss: 9.120458386534004\n",
            "          vf_explained_var: -3.8997524528093234e-05\n",
            "          vf_loss: 9.135916686314408\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_agent_steps_trained: 4000\n",
            "    num_env_steps_sampled: 4000\n",
            "    num_env_steps_trained: 4000\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 4000\n",
            "  num_agent_steps_trained: 4000\n",
            "  num_env_steps_sampled: 4000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 4000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 87.14117647058823\n",
            "    ram_util_percent: 25.299999999999997\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15183936703314543\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14560922912502938\n",
            "    mean_inference_ms: 2.311839683368363\n",
            "    mean_raw_obs_processing_ms: 0.9179244227943528\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 20.88421052631579\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 84.0\n",
            "    episode_reward_mean: 20.88421052631579\n",
            "    episode_reward_min: 8.0\n",
            "    episodes_this_iter: 190\n",
            "    hist_stats:\n",
            "      episode_lengths: [16, 10, 10, 32, 18, 15, 16, 36, 17, 19, 21, 30, 14, 23, 28,\n",
            "        41, 30, 46, 13, 14, 14, 19, 18, 12, 18, 14, 17, 17, 20, 24, 38, 18, 10, 12,\n",
            "        15, 27, 26, 25, 23, 20, 16, 25, 13, 37, 18, 18, 41, 13, 23, 17, 17, 21, 38,\n",
            "        16, 20, 15, 22, 11, 15, 17, 11, 13, 10, 44, 14, 19, 14, 19, 22, 13, 16, 14,\n",
            "        20, 21, 12, 21, 14, 10, 29, 18, 12, 15, 10, 12, 40, 15, 33, 22, 21, 14, 14,\n",
            "        8, 21, 11, 22, 18, 20, 17, 26, 23, 20, 15, 13, 19, 26, 28, 13, 20, 25, 11, 28,\n",
            "        17, 15, 23, 14, 37, 34, 30, 22, 39, 24, 10, 20, 30, 20, 29, 22, 47, 13, 25,\n",
            "        22, 9, 14, 18, 20, 14, 18, 16, 16, 17, 13, 29, 27, 28, 19, 20, 24, 19, 24, 11,\n",
            "        13, 17, 24, 20, 11, 33, 10, 28, 31, 14, 24, 14, 37, 17, 21, 13, 44, 15, 37,\n",
            "        13, 11, 16, 12, 25, 30, 19, 23, 28, 33, 15, 21, 13, 84, 11, 11, 19, 68, 19,\n",
            "        12, 18]\n",
            "      episode_reward: [16.0, 10.0, 10.0, 32.0, 18.0, 15.0, 16.0, 36.0, 17.0, 19.0, 21.0,\n",
            "        30.0, 14.0, 23.0, 28.0, 41.0, 30.0, 46.0, 13.0, 14.0, 14.0, 19.0, 18.0, 12.0,\n",
            "        18.0, 14.0, 17.0, 17.0, 20.0, 24.0, 38.0, 18.0, 10.0, 12.0, 15.0, 27.0, 26.0,\n",
            "        25.0, 23.0, 20.0, 16.0, 25.0, 13.0, 37.0, 18.0, 18.0, 41.0, 13.0, 23.0, 17.0,\n",
            "        17.0, 21.0, 38.0, 16.0, 20.0, 15.0, 22.0, 11.0, 15.0, 17.0, 11.0, 13.0, 10.0,\n",
            "        44.0, 14.0, 19.0, 14.0, 19.0, 22.0, 13.0, 16.0, 14.0, 20.0, 21.0, 12.0, 21.0,\n",
            "        14.0, 10.0, 29.0, 18.0, 12.0, 15.0, 10.0, 12.0, 40.0, 15.0, 33.0, 22.0, 21.0,\n",
            "        14.0, 14.0, 8.0, 21.0, 11.0, 22.0, 18.0, 20.0, 17.0, 26.0, 23.0, 20.0, 15.0,\n",
            "        13.0, 19.0, 26.0, 28.0, 13.0, 20.0, 25.0, 11.0, 28.0, 17.0, 15.0, 23.0, 14.0,\n",
            "        37.0, 34.0, 30.0, 22.0, 39.0, 24.0, 10.0, 20.0, 30.0, 20.0, 29.0, 22.0, 47.0,\n",
            "        13.0, 25.0, 22.0, 9.0, 14.0, 18.0, 20.0, 14.0, 18.0, 16.0, 16.0, 17.0, 13.0,\n",
            "        29.0, 27.0, 28.0, 19.0, 20.0, 24.0, 19.0, 24.0, 11.0, 13.0, 17.0, 24.0, 20.0,\n",
            "        11.0, 33.0, 10.0, 28.0, 31.0, 14.0, 24.0, 14.0, 37.0, 17.0, 21.0, 13.0, 44.0,\n",
            "        15.0, 37.0, 13.0, 11.0, 16.0, 12.0, 25.0, 30.0, 19.0, 23.0, 28.0, 33.0, 15.0,\n",
            "        21.0, 13.0, 84.0, 11.0, 11.0, 19.0, 68.0, 19.0, 12.0, 18.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.15183936703314543\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.14560922912502938\n",
            "      mean_inference_ms: 2.311839683368363\n",
            "      mean_raw_obs_processing_ms: 0.9179244227943528\n",
            "  time_since_restore: 11.711516380310059\n",
            "  time_this_iter_s: 11.711516380310059\n",
            "  time_total_s: 11.711516380310059\n",
            "  timers:\n",
            "    learn_throughput: 915.706\n",
            "    learn_time_ms: 4368.212\n",
            "    load_throughput: 8555439.062\n",
            "    load_time_ms: 0.468\n",
            "    synch_weights_time_ms: 3.297\n",
            "    training_iteration_time_ms: 11703.3\n",
            "  timestamp: 1700090981\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 4000\n",
            "  training_iteration: 1\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 8000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-29-50\n",
            "  done: false\n",
            "  episode_len_mean: 30.692307692307693\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 104.0\n",
            "  episode_reward_mean: 30.692307692307693\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 130\n",
            "  episodes_total: 320\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6629640535000831\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.7339806261242077\n",
            "          kl: 0.007245998214870365\n",
            "          policy_loss: -0.012433933167247682\n",
            "          total_loss: 9.334420020605927\n",
            "          vf_explained_var: -0.00021364528645751298\n",
            "          vf_loss: 9.345404789011965\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 8000\n",
            "  num_agent_steps_trained: 8000\n",
            "  num_env_steps_sampled: 8000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 8000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 86.23846153846154\n",
            "    ram_util_percent: 25.399999999999995\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11802771012023465\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11275226279190466\n",
            "    mean_inference_ms: 1.8219911346016038\n",
            "    mean_raw_obs_processing_ms: 0.7071647587570223\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 30.692307692307693\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 104.0\n",
            "    episode_reward_mean: 30.692307692307693\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 130\n",
            "    hist_stats:\n",
            "      episode_lengths: [31, 15, 78, 12, 21, 15, 22, 70, 16, 22, 40, 25, 15, 30, 20,\n",
            "        14, 73, 45, 15, 95, 51, 22, 15, 19, 13, 27, 47, 16, 15, 34, 60, 20, 26, 32,\n",
            "        20, 104, 24, 27, 26, 19, 74, 21, 40, 31, 58, 70, 21, 29, 12, 14, 29, 20, 24,\n",
            "        39, 13, 17, 16, 27, 18, 33, 22, 29, 34, 34, 15, 58, 21, 14, 14, 24, 16, 65,\n",
            "        25, 73, 27, 18, 21, 18, 15, 33, 63, 26, 16, 25, 43, 18, 23, 21, 21, 72, 29,\n",
            "        65, 13, 22, 43, 28, 22, 26, 12, 21, 18, 19, 21, 13, 37, 30, 31, 23, 24, 80,\n",
            "        20, 50, 12, 28, 16, 31, 69, 39, 15, 47, 20, 20, 35, 68, 40, 32, 19, 12, 13,\n",
            "        26]\n",
            "      episode_reward: [31.0, 15.0, 78.0, 12.0, 21.0, 15.0, 22.0, 70.0, 16.0, 22.0, 40.0,\n",
            "        25.0, 15.0, 30.0, 20.0, 14.0, 73.0, 45.0, 15.0, 95.0, 51.0, 22.0, 15.0, 19.0,\n",
            "        13.0, 27.0, 47.0, 16.0, 15.0, 34.0, 60.0, 20.0, 26.0, 32.0, 20.0, 104.0, 24.0,\n",
            "        27.0, 26.0, 19.0, 74.0, 21.0, 40.0, 31.0, 58.0, 70.0, 21.0, 29.0, 12.0, 14.0,\n",
            "        29.0, 20.0, 24.0, 39.0, 13.0, 17.0, 16.0, 27.0, 18.0, 33.0, 22.0, 29.0, 34.0,\n",
            "        34.0, 15.0, 58.0, 21.0, 14.0, 14.0, 24.0, 16.0, 65.0, 25.0, 73.0, 27.0, 18.0,\n",
            "        21.0, 18.0, 15.0, 33.0, 63.0, 26.0, 16.0, 25.0, 43.0, 18.0, 23.0, 21.0, 21.0,\n",
            "        72.0, 29.0, 65.0, 13.0, 22.0, 43.0, 28.0, 22.0, 26.0, 12.0, 21.0, 18.0, 19.0,\n",
            "        21.0, 13.0, 37.0, 30.0, 31.0, 23.0, 24.0, 80.0, 20.0, 50.0, 12.0, 28.0, 16.0,\n",
            "        31.0, 69.0, 39.0, 15.0, 47.0, 20.0, 20.0, 35.0, 68.0, 40.0, 32.0, 19.0, 12.0,\n",
            "        13.0, 26.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.11802771012023465\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11275226279190466\n",
            "      mean_inference_ms: 1.8219911346016038\n",
            "      mean_raw_obs_processing_ms: 0.7071647587570223\n",
            "  time_since_restore: 20.369727849960327\n",
            "  time_this_iter_s: 8.658211469650269\n",
            "  time_total_s: 20.369727849960327\n",
            "  timers:\n",
            "    learn_throughput: 894.129\n",
            "    learn_time_ms: 4473.628\n",
            "    load_throughput: 9451952.676\n",
            "    load_time_ms: 0.423\n",
            "    synch_weights_time_ms: 2.457\n",
            "    training_iteration_time_ms: 10175.562\n",
            "  timestamp: 1700090990\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 8000\n",
            "  training_iteration: 2\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 12000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_agent_steps_trained: 12000\n",
            "    num_env_steps_sampled: 12000\n",
            "    num_env_steps_trained: 12000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-01\n",
            "  done: false\n",
            "  episode_len_mean: 34.34782608695652\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 132.0\n",
            "  episode_reward_mean: 34.34782608695652\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 115\n",
            "  episodes_total: 435\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6334137446777794\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6234628615199879\n",
            "          kl: 0.006413510306746251\n",
            "          policy_loss: -0.014278308880986065\n",
            "          total_loss: 9.34166624161505\n",
            "          vf_explained_var: -0.00018193805089560889\n",
            "          vf_loss: 9.35466184308452\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_agent_steps_trained: 12000\n",
            "    num_env_steps_sampled: 12000\n",
            "    num_env_steps_trained: 12000\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 12000\n",
            "  num_agent_steps_trained: 12000\n",
            "  num_env_steps_sampled: 12000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 12000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 85.02857142857142\n",
            "    ram_util_percent: 25.485714285714288\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.12114585686646186\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.12028592900552104\n",
            "    mean_inference_ms: 1.9285590120952896\n",
            "    mean_raw_obs_processing_ms: 0.7267203828859973\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 34.34782608695652\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 132.0\n",
            "    episode_reward_mean: 34.34782608695652\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 115\n",
            "    hist_stats:\n",
            "      episode_lengths: [22, 32, 26, 30, 31, 23, 73, 21, 52, 31, 50, 16, 48, 44, 80,\n",
            "        15, 43, 17, 27, 22, 26, 64, 20, 57, 27, 37, 31, 16, 14, 27, 14, 43, 25, 34,\n",
            "        19, 20, 93, 79, 47, 19, 63, 33, 50, 63, 45, 18, 33, 32, 12, 51, 18, 21, 15,\n",
            "        78, 39, 40, 90, 24, 51, 53, 20, 19, 28, 13, 16, 15, 16, 35, 17, 60, 18, 31,\n",
            "        23, 17, 31, 19, 18, 20, 33, 27, 50, 70, 22, 16, 31, 47, 20, 14, 86, 13, 63,\n",
            "        21, 17, 33, 16, 13, 48, 22, 34, 23, 13, 132, 51, 47, 26, 33, 16, 26, 20, 28,\n",
            "        38, 36, 35, 28, 42]\n",
            "      episode_reward: [22.0, 32.0, 26.0, 30.0, 31.0, 23.0, 73.0, 21.0, 52.0, 31.0, 50.0,\n",
            "        16.0, 48.0, 44.0, 80.0, 15.0, 43.0, 17.0, 27.0, 22.0, 26.0, 64.0, 20.0, 57.0,\n",
            "        27.0, 37.0, 31.0, 16.0, 14.0, 27.0, 14.0, 43.0, 25.0, 34.0, 19.0, 20.0, 93.0,\n",
            "        79.0, 47.0, 19.0, 63.0, 33.0, 50.0, 63.0, 45.0, 18.0, 33.0, 32.0, 12.0, 51.0,\n",
            "        18.0, 21.0, 15.0, 78.0, 39.0, 40.0, 90.0, 24.0, 51.0, 53.0, 20.0, 19.0, 28.0,\n",
            "        13.0, 16.0, 15.0, 16.0, 35.0, 17.0, 60.0, 18.0, 31.0, 23.0, 17.0, 31.0, 19.0,\n",
            "        18.0, 20.0, 33.0, 27.0, 50.0, 70.0, 22.0, 16.0, 31.0, 47.0, 20.0, 14.0, 86.0,\n",
            "        13.0, 63.0, 21.0, 17.0, 33.0, 16.0, 13.0, 48.0, 22.0, 34.0, 23.0, 13.0, 132.0,\n",
            "        51.0, 47.0, 26.0, 33.0, 16.0, 26.0, 20.0, 28.0, 38.0, 36.0, 35.0, 28.0, 42.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.12114585686646186\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.12028592900552104\n",
            "      mean_inference_ms: 1.9285590120952896\n",
            "      mean_raw_obs_processing_ms: 0.7267203828859973\n",
            "  time_since_restore: 30.86237907409668\n",
            "  time_this_iter_s: 10.492651224136353\n",
            "  time_total_s: 30.86237907409668\n",
            "  timers:\n",
            "    learn_throughput: 937.049\n",
            "    learn_time_ms: 4268.719\n",
            "    load_throughput: 9369256.888\n",
            "    load_time_ms: 0.427\n",
            "    synch_weights_time_ms: 2.208\n",
            "    training_iteration_time_ms: 10278.474\n",
            "  timestamp: 1700091001\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 12000\n",
            "  training_iteration: 3\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 16000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_agent_steps_trained: 16000\n",
            "    num_env_steps_sampled: 16000\n",
            "    num_env_steps_trained: 16000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-15\n",
            "  done: false\n",
            "  episode_len_mean: 52.35\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 52.35\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 66\n",
            "  episodes_total: 501\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6275225401565593\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5085896410769032\n",
            "          kl: 0.003457480286426215\n",
            "          policy_loss: -0.014465101196440637\n",
            "          total_loss: 9.519040363065658\n",
            "          vf_explained_var: -0.0009010186118464317\n",
            "          vf_loss: 9.532813964351531\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_agent_steps_trained: 16000\n",
            "    num_env_steps_sampled: 16000\n",
            "    num_env_steps_trained: 16000\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 16000\n",
            "  num_agent_steps_trained: 16000\n",
            "  num_env_steps_sampled: 16000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 16000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 92.715\n",
            "    ram_util_percent: 25.485\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11428794163441232\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11360536277919614\n",
            "    mean_inference_ms: 1.831077664948711\n",
            "    mean_raw_obs_processing_ms: 0.6866952348730251\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 52.35\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 52.35\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 66\n",
            "    hist_stats:\n",
            "      episode_lengths: [70, 22, 16, 31, 47, 20, 14, 86, 13, 63, 21, 17, 33, 16, 13,\n",
            "        48, 22, 34, 23, 13, 132, 51, 47, 26, 33, 16, 26, 20, 28, 38, 36, 35, 28, 42,\n",
            "        74, 19, 20, 18, 14, 22, 108, 139, 23, 21, 45, 76, 11, 57, 36, 20, 84, 33, 59,\n",
            "        130, 18, 59, 50, 23, 39, 79, 53, 179, 61, 46, 82, 55, 90, 35, 32, 144, 40, 156,\n",
            "        26, 54, 30, 139, 39, 12, 14, 20, 42, 200, 71, 35, 186, 26, 54, 84, 115, 49,\n",
            "        104, 87, 28, 38, 70, 104, 24, 16, 90, 48]\n",
            "      episode_reward: [70.0, 22.0, 16.0, 31.0, 47.0, 20.0, 14.0, 86.0, 13.0, 63.0, 21.0,\n",
            "        17.0, 33.0, 16.0, 13.0, 48.0, 22.0, 34.0, 23.0, 13.0, 132.0, 51.0, 47.0, 26.0,\n",
            "        33.0, 16.0, 26.0, 20.0, 28.0, 38.0, 36.0, 35.0, 28.0, 42.0, 74.0, 19.0, 20.0,\n",
            "        18.0, 14.0, 22.0, 108.0, 139.0, 23.0, 21.0, 45.0, 76.0, 11.0, 57.0, 36.0, 20.0,\n",
            "        84.0, 33.0, 59.0, 130.0, 18.0, 59.0, 50.0, 23.0, 39.0, 79.0, 53.0, 179.0, 61.0,\n",
            "        46.0, 82.0, 55.0, 90.0, 35.0, 32.0, 144.0, 40.0, 156.0, 26.0, 54.0, 30.0, 139.0,\n",
            "        39.0, 12.0, 14.0, 20.0, 42.0, 200.0, 71.0, 35.0, 186.0, 26.0, 54.0, 84.0, 115.0,\n",
            "        49.0, 104.0, 87.0, 28.0, 38.0, 70.0, 104.0, 24.0, 16.0, 90.0, 48.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.11428794163441232\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11360536277919614\n",
            "      mean_inference_ms: 1.831077664948711\n",
            "      mean_raw_obs_processing_ms: 0.6866952348730251\n",
            "  time_since_restore: 44.65443992614746\n",
            "  time_this_iter_s: 13.792060852050781\n",
            "  time_total_s: 44.65443992614746\n",
            "  timers:\n",
            "    learn_throughput: 712.183\n",
            "    learn_time_ms: 5616.537\n",
            "    load_throughput: 9098273.319\n",
            "    load_time_ms: 0.44\n",
            "    synch_weights_time_ms: 2.437\n",
            "    training_iteration_time_ms: 11153.949\n",
            "  timestamp: 1700091015\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 16000\n",
            "  training_iteration: 4\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 20000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_agent_steps_trained: 20000\n",
            "    num_env_steps_sampled: 20000\n",
            "    num_env_steps_trained: 20000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-23\n",
            "  done: false\n",
            "  episode_len_mean: 74.5\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 74.5\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 46\n",
            "  episodes_total: 547\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.10000000000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6128060032603562\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.4823613942710943\n",
            "          kl: 0.0039087303373222995\n",
            "          policy_loss: -0.014146035769453613\n",
            "          total_loss: 9.602656026040354\n",
            "          vf_explained_var: -0.006146479486137308\n",
            "          vf_loss: 9.616411199877339\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_agent_steps_trained: 20000\n",
            "    num_env_steps_sampled: 20000\n",
            "    num_env_steps_trained: 20000\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 20000\n",
            "  num_agent_steps_trained: 20000\n",
            "  num_env_steps_sampled: 20000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 20000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 79.46666666666668\n",
            "    ram_util_percent: 25.5\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10998270206460209\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1082226872267797\n",
            "    mean_inference_ms: 1.749404922093188\n",
            "    mean_raw_obs_processing_ms: 0.6522826397432929\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 74.5\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 74.5\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 46\n",
            "    hist_stats:\n",
            "      episode_lengths: [11, 57, 36, 20, 84, 33, 59, 130, 18, 59, 50, 23, 39, 79, 53,\n",
            "        179, 61, 46, 82, 55, 90, 35, 32, 144, 40, 156, 26, 54, 30, 139, 39, 12, 14,\n",
            "        20, 42, 200, 71, 35, 186, 26, 54, 84, 115, 49, 104, 87, 28, 38, 70, 104, 24,\n",
            "        16, 90, 48, 154, 31, 160, 101, 29, 93, 73, 181, 121, 29, 116, 174, 115, 88,\n",
            "        48, 200, 187, 27, 22, 32, 24, 129, 55, 98, 79, 18, 103, 51, 33, 130, 94, 107,\n",
            "        31, 68, 32, 106, 133, 29, 128, 47, 48, 131, 18, 76, 25, 200]\n",
            "      episode_reward: [11.0, 57.0, 36.0, 20.0, 84.0, 33.0, 59.0, 130.0, 18.0, 59.0,\n",
            "        50.0, 23.0, 39.0, 79.0, 53.0, 179.0, 61.0, 46.0, 82.0, 55.0, 90.0, 35.0, 32.0,\n",
            "        144.0, 40.0, 156.0, 26.0, 54.0, 30.0, 139.0, 39.0, 12.0, 14.0, 20.0, 42.0, 200.0,\n",
            "        71.0, 35.0, 186.0, 26.0, 54.0, 84.0, 115.0, 49.0, 104.0, 87.0, 28.0, 38.0, 70.0,\n",
            "        104.0, 24.0, 16.0, 90.0, 48.0, 154.0, 31.0, 160.0, 101.0, 29.0, 93.0, 73.0,\n",
            "        181.0, 121.0, 29.0, 116.0, 174.0, 115.0, 88.0, 48.0, 200.0, 187.0, 27.0, 22.0,\n",
            "        32.0, 24.0, 129.0, 55.0, 98.0, 79.0, 18.0, 103.0, 51.0, 33.0, 130.0, 94.0, 107.0,\n",
            "        31.0, 68.0, 32.0, 106.0, 133.0, 29.0, 128.0, 47.0, 48.0, 131.0, 18.0, 76.0,\n",
            "        25.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10998270206460209\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1082226872267797\n",
            "      mean_inference_ms: 1.749404922093188\n",
            "      mean_raw_obs_processing_ms: 0.6522826397432929\n",
            "  time_since_restore: 52.817142963409424\n",
            "  time_this_iter_s: 8.162703037261963\n",
            "  time_total_s: 52.817142963409424\n",
            "  timers:\n",
            "    learn_throughput: 757.336\n",
            "    learn_time_ms: 5281.672\n",
            "    load_throughput: 9258949.227\n",
            "    load_time_ms: 0.432\n",
            "    synch_weights_time_ms: 2.454\n",
            "    training_iteration_time_ms: 10554.177\n",
            "  timestamp: 1700091023\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 20000\n",
            "  training_iteration: 5\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 24000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_agent_steps_trained: 24000\n",
            "    num_env_steps_sampled: 24000\n",
            "    num_env_steps_trained: 24000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-34\n",
            "  done: false\n",
            "  episode_len_mean: 92.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 92.4\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 30\n",
            "  episodes_total: 577\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.05000000000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6038806858242199\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.50333702154057\n",
            "          kl: 0.001604537949268092\n",
            "          policy_loss: -0.010950850174632125\n",
            "          total_loss: 9.700343036651612\n",
            "          vf_explained_var: 0.00045272457984185985\n",
            "          vf_loss: 9.711213684082031\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_agent_steps_trained: 24000\n",
            "    num_env_steps_sampled: 24000\n",
            "    num_env_steps_trained: 24000\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 24000\n",
            "  num_agent_steps_trained: 24000\n",
            "  num_env_steps_sampled: 24000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 24000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 96.13125\n",
            "    ram_util_percent: 25.58125\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10866434068032493\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10668494950597036\n",
            "    mean_inference_ms: 1.7299077319561997\n",
            "    mean_raw_obs_processing_ms: 0.6398269101648352\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 92.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 92.4\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 30\n",
            "    hist_stats:\n",
            "      episode_lengths: [39, 12, 14, 20, 42, 200, 71, 35, 186, 26, 54, 84, 115, 49, 104,\n",
            "        87, 28, 38, 70, 104, 24, 16, 90, 48, 154, 31, 160, 101, 29, 93, 73, 181, 121,\n",
            "        29, 116, 174, 115, 88, 48, 200, 187, 27, 22, 32, 24, 129, 55, 98, 79, 18, 103,\n",
            "        51, 33, 130, 94, 107, 31, 68, 32, 106, 133, 29, 128, 47, 48, 131, 18, 76, 25,\n",
            "        200, 161, 163, 132, 200, 141, 108, 149, 153, 160, 171, 80, 13, 184, 58, 200,\n",
            "        82, 191, 200, 113, 139, 87, 129, 25, 177, 52, 64, 145, 134, 78, 21]\n",
            "      episode_reward: [39.0, 12.0, 14.0, 20.0, 42.0, 200.0, 71.0, 35.0, 186.0, 26.0,\n",
            "        54.0, 84.0, 115.0, 49.0, 104.0, 87.0, 28.0, 38.0, 70.0, 104.0, 24.0, 16.0, 90.0,\n",
            "        48.0, 154.0, 31.0, 160.0, 101.0, 29.0, 93.0, 73.0, 181.0, 121.0, 29.0, 116.0,\n",
            "        174.0, 115.0, 88.0, 48.0, 200.0, 187.0, 27.0, 22.0, 32.0, 24.0, 129.0, 55.0,\n",
            "        98.0, 79.0, 18.0, 103.0, 51.0, 33.0, 130.0, 94.0, 107.0, 31.0, 68.0, 32.0, 106.0,\n",
            "        133.0, 29.0, 128.0, 47.0, 48.0, 131.0, 18.0, 76.0, 25.0, 200.0, 161.0, 163.0,\n",
            "        132.0, 200.0, 141.0, 108.0, 149.0, 153.0, 160.0, 171.0, 80.0, 13.0, 184.0, 58.0,\n",
            "        200.0, 82.0, 191.0, 200.0, 113.0, 139.0, 87.0, 129.0, 25.0, 177.0, 52.0, 64.0,\n",
            "        145.0, 134.0, 78.0, 21.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10866434068032493\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10668494950597036\n",
            "      mean_inference_ms: 1.7299077319561997\n",
            "      mean_raw_obs_processing_ms: 0.6398269101648352\n",
            "  time_since_restore: 63.871562480926514\n",
            "  time_this_iter_s: 11.05441951751709\n",
            "  time_total_s: 63.871562480926514\n",
            "  timers:\n",
            "    learn_throughput: 752.792\n",
            "    learn_time_ms: 5313.552\n",
            "    load_throughput: 8499096.251\n",
            "    load_time_ms: 0.471\n",
            "    synch_weights_time_ms: 2.309\n",
            "    training_iteration_time_ms: 10636.809\n",
            "  timestamp: 1700091034\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 24000\n",
            "  training_iteration: 6\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 28000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_agent_steps_trained: 28000\n",
            "    num_env_steps_sampled: 28000\n",
            "    num_env_steps_trained: 28000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-42\n",
            "  done: false\n",
            "  episode_len_mean: 117.05\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 117.05\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 603\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.025000000000000005\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5976992321270769\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.532085710227169\n",
            "          kl: 0.0002588060803756743\n",
            "          policy_loss: -0.010079086812273149\n",
            "          total_loss: 9.706573232014973\n",
            "          vf_explained_var: 0.002735356810272381\n",
            "          vf_loss: 9.716645862210182\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_agent_steps_trained: 28000\n",
            "    num_env_steps_sampled: 28000\n",
            "    num_env_steps_trained: 28000\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 28000\n",
            "  num_agent_steps_trained: 28000\n",
            "  num_env_steps_sampled: 28000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 28000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 78.56363636363636\n",
            "    ram_util_percent: 25.6\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10807323615288539\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1050979489505918\n",
            "    mean_inference_ms: 1.7050194405657617\n",
            "    mean_raw_obs_processing_ms: 0.6277661526316907\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 117.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 117.05\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 26\n",
            "    hist_stats:\n",
            "      episode_lengths: [160, 101, 29, 93, 73, 181, 121, 29, 116, 174, 115, 88, 48, 200,\n",
            "        187, 27, 22, 32, 24, 129, 55, 98, 79, 18, 103, 51, 33, 130, 94, 107, 31, 68,\n",
            "        32, 106, 133, 29, 128, 47, 48, 131, 18, 76, 25, 200, 161, 163, 132, 200, 141,\n",
            "        108, 149, 153, 160, 171, 80, 13, 184, 58, 200, 82, 191, 200, 113, 139, 87, 129,\n",
            "        25, 177, 52, 64, 145, 134, 78, 21, 200, 150, 191, 119, 177, 200, 165, 200, 200,\n",
            "        190, 169, 131, 200, 185, 42, 135, 143, 151, 104, 177, 150, 124, 200, 200, 133,\n",
            "        170]\n",
            "      episode_reward: [160.0, 101.0, 29.0, 93.0, 73.0, 181.0, 121.0, 29.0, 116.0, 174.0,\n",
            "        115.0, 88.0, 48.0, 200.0, 187.0, 27.0, 22.0, 32.0, 24.0, 129.0, 55.0, 98.0,\n",
            "        79.0, 18.0, 103.0, 51.0, 33.0, 130.0, 94.0, 107.0, 31.0, 68.0, 32.0, 106.0,\n",
            "        133.0, 29.0, 128.0, 47.0, 48.0, 131.0, 18.0, 76.0, 25.0, 200.0, 161.0, 163.0,\n",
            "        132.0, 200.0, 141.0, 108.0, 149.0, 153.0, 160.0, 171.0, 80.0, 13.0, 184.0, 58.0,\n",
            "        200.0, 82.0, 191.0, 200.0, 113.0, 139.0, 87.0, 129.0, 25.0, 177.0, 52.0, 64.0,\n",
            "        145.0, 134.0, 78.0, 21.0, 200.0, 150.0, 191.0, 119.0, 177.0, 200.0, 165.0, 200.0,\n",
            "        200.0, 190.0, 169.0, 131.0, 200.0, 185.0, 42.0, 135.0, 143.0, 151.0, 104.0,\n",
            "        177.0, 150.0, 124.0, 200.0, 200.0, 133.0, 170.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10807323615288539\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1050979489505918\n",
            "      mean_inference_ms: 1.7050194405657617\n",
            "      mean_raw_obs_processing_ms: 0.6277661526316907\n",
            "  time_since_restore: 71.97187972068787\n",
            "  time_this_iter_s: 8.100317239761353\n",
            "  time_total_s: 71.97187972068787\n",
            "  timers:\n",
            "    learn_throughput: 780.937\n",
            "    learn_time_ms: 5122.051\n",
            "    load_throughput: 8035614.916\n",
            "    load_time_ms: 0.498\n",
            "    synch_weights_time_ms: 2.249\n",
            "    training_iteration_time_ms: 10273.542\n",
            "  timestamp: 1700091042\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 28000\n",
            "  training_iteration: 7\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 32000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_agent_steps_trained: 32000\n",
            "    num_env_steps_sampled: 32000\n",
            "    num_env_steps_trained: 32000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-30-53\n",
            "  done: false\n",
            "  episode_len_mean: 132.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 132.01\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 27\n",
            "  episodes_total: 630\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.012500000000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5879027990884679\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5303945104200993\n",
            "          kl: 0.0024994454137851688\n",
            "          policy_loss: -0.011291143971104776\n",
            "          total_loss: 9.662161540985107\n",
            "          vf_explained_var: 0.005621688340299873\n",
            "          vf_loss: 9.673421450584165\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_agent_steps_trained: 32000\n",
            "    num_env_steps_sampled: 32000\n",
            "    num_env_steps_trained: 32000\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 32000\n",
            "  num_agent_steps_trained: 32000\n",
            "  num_env_steps_sampled: 32000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 32000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 92.56875\n",
            "    ram_util_percent: 25.5375\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10816749282929802\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1051852132415608\n",
            "    mean_inference_ms: 1.7071813332116772\n",
            "    mean_raw_obs_processing_ms: 0.6252179539639178\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 132.01\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 132.01\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 27\n",
            "    hist_stats:\n",
            "      episode_lengths: [130, 94, 107, 31, 68, 32, 106, 133, 29, 128, 47, 48, 131, 18,\n",
            "        76, 25, 200, 161, 163, 132, 200, 141, 108, 149, 153, 160, 171, 80, 13, 184,\n",
            "        58, 200, 82, 191, 200, 113, 139, 87, 129, 25, 177, 52, 64, 145, 134, 78, 21,\n",
            "        200, 150, 191, 119, 177, 200, 165, 200, 200, 190, 169, 131, 200, 185, 42, 135,\n",
            "        143, 151, 104, 177, 150, 124, 200, 200, 133, 170, 200, 200, 191, 200, 150, 24,\n",
            "        154, 118, 152, 63, 164, 159, 136, 132, 135, 133, 200, 23, 159, 200, 145, 96,\n",
            "        44, 200, 194, 150, 160]\n",
            "      episode_reward: [130.0, 94.0, 107.0, 31.0, 68.0, 32.0, 106.0, 133.0, 29.0, 128.0,\n",
            "        47.0, 48.0, 131.0, 18.0, 76.0, 25.0, 200.0, 161.0, 163.0, 132.0, 200.0, 141.0,\n",
            "        108.0, 149.0, 153.0, 160.0, 171.0, 80.0, 13.0, 184.0, 58.0, 200.0, 82.0, 191.0,\n",
            "        200.0, 113.0, 139.0, 87.0, 129.0, 25.0, 177.0, 52.0, 64.0, 145.0, 134.0, 78.0,\n",
            "        21.0, 200.0, 150.0, 191.0, 119.0, 177.0, 200.0, 165.0, 200.0, 200.0, 190.0,\n",
            "        169.0, 131.0, 200.0, 185.0, 42.0, 135.0, 143.0, 151.0, 104.0, 177.0, 150.0,\n",
            "        124.0, 200.0, 200.0, 133.0, 170.0, 200.0, 200.0, 191.0, 200.0, 150.0, 24.0,\n",
            "        154.0, 118.0, 152.0, 63.0, 164.0, 159.0, 136.0, 132.0, 135.0, 133.0, 200.0,\n",
            "        23.0, 159.0, 200.0, 145.0, 96.0, 44.0, 200.0, 194.0, 150.0, 160.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10816749282929802\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1051852132415608\n",
            "      mean_inference_ms: 1.7071813332116772\n",
            "      mean_raw_obs_processing_ms: 0.6252179539639178\n",
            "  time_since_restore: 83.13105511665344\n",
            "  time_this_iter_s: 11.159175395965576\n",
            "  time_total_s: 83.13105511665344\n",
            "  timers:\n",
            "    learn_throughput: 783.32\n",
            "    learn_time_ms: 5106.47\n",
            "    load_throughput: 7771727.157\n",
            "    load_time_ms: 0.515\n",
            "    synch_weights_time_ms: 2.285\n",
            "    training_iteration_time_ms: 10383.341\n",
            "  timestamp: 1700091053\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 32000\n",
            "  training_iteration: 8\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 36000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_agent_steps_trained: 36000\n",
            "    num_env_steps_sampled: 36000\n",
            "    num_env_steps_trained: 36000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-01\n",
            "  done: false\n",
            "  episode_len_mean: 147.6\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 147.6\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 654\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.006250000000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5747362286172887\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.42367423554902434\n",
            "          kl: 0.0022078814652627577\n",
            "          policy_loss: -0.010254743877517921\n",
            "          total_loss: 9.672332164805422\n",
            "          vf_explained_var: 0.0012284318606058757\n",
            "          vf_loss: 9.682573119542932\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_agent_steps_trained: 36000\n",
            "    num_env_steps_sampled: 36000\n",
            "    num_env_steps_trained: 36000\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 36000\n",
            "  num_agent_steps_trained: 36000\n",
            "  num_env_steps_sampled: 36000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 36000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 78.24545454545455\n",
            "    ram_util_percent: 25.5\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10805615052821108\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1046848982862088\n",
            "    mean_inference_ms: 1.6999249650804893\n",
            "    mean_raw_obs_processing_ms: 0.6201054961175867\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 147.6\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 147.6\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 24\n",
            "    hist_stats:\n",
            "      episode_lengths: [153, 160, 171, 80, 13, 184, 58, 200, 82, 191, 200, 113, 139,\n",
            "        87, 129, 25, 177, 52, 64, 145, 134, 78, 21, 200, 150, 191, 119, 177, 200, 165,\n",
            "        200, 200, 190, 169, 131, 200, 185, 42, 135, 143, 151, 104, 177, 150, 124, 200,\n",
            "        200, 133, 170, 200, 200, 191, 200, 150, 24, 154, 118, 152, 63, 164, 159, 136,\n",
            "        132, 135, 133, 200, 23, 159, 200, 145, 96, 44, 200, 194, 150, 160, 200, 155,\n",
            "        136, 74, 138, 200, 200, 166, 191, 165, 138, 169, 111, 200, 135, 200, 200, 166,\n",
            "        141, 200, 170, 168, 193, 200]\n",
            "      episode_reward: [153.0, 160.0, 171.0, 80.0, 13.0, 184.0, 58.0, 200.0, 82.0, 191.0,\n",
            "        200.0, 113.0, 139.0, 87.0, 129.0, 25.0, 177.0, 52.0, 64.0, 145.0, 134.0, 78.0,\n",
            "        21.0, 200.0, 150.0, 191.0, 119.0, 177.0, 200.0, 165.0, 200.0, 200.0, 190.0,\n",
            "        169.0, 131.0, 200.0, 185.0, 42.0, 135.0, 143.0, 151.0, 104.0, 177.0, 150.0,\n",
            "        124.0, 200.0, 200.0, 133.0, 170.0, 200.0, 200.0, 191.0, 200.0, 150.0, 24.0,\n",
            "        154.0, 118.0, 152.0, 63.0, 164.0, 159.0, 136.0, 132.0, 135.0, 133.0, 200.0,\n",
            "        23.0, 159.0, 200.0, 145.0, 96.0, 44.0, 200.0, 194.0, 150.0, 160.0, 200.0, 155.0,\n",
            "        136.0, 74.0, 138.0, 200.0, 200.0, 166.0, 191.0, 165.0, 138.0, 169.0, 111.0,\n",
            "        200.0, 135.0, 200.0, 200.0, 166.0, 141.0, 200.0, 170.0, 168.0, 193.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10805615052821108\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1046848982862088\n",
            "      mean_inference_ms: 1.6999249650804893\n",
            "      mean_raw_obs_processing_ms: 0.6201054961175867\n",
            "  time_since_restore: 91.12220215797424\n",
            "  time_this_iter_s: 7.991147041320801\n",
            "  time_total_s: 91.12220215797424\n",
            "  timers:\n",
            "    learn_throughput: 804.605\n",
            "    learn_time_ms: 4971.382\n",
            "    load_throughput: 7948775.742\n",
            "    load_time_ms: 0.503\n",
            "    synch_weights_time_ms: 2.25\n",
            "    training_iteration_time_ms: 10116.738\n",
            "  timestamp: 1700091061\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 36000\n",
            "  training_iteration: 9\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 40000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_agent_steps_trained: 40000\n",
            "    num_env_steps_sampled: 40000\n",
            "    num_env_steps_trained: 40000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-13\n",
            "  done: false\n",
            "  episode_len_mean: 157.94\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 157.94\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 679\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0031250000000000006\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5774868063388332\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.4562199638335295\n",
            "          kl: 0.00024045196382533525\n",
            "          policy_loss: -0.009213409424629263\n",
            "          total_loss: 9.635616978265906\n",
            "          vf_explained_var: -0.000345066285902454\n",
            "          vf_loss: 9.644829701864591\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_agent_steps_trained: 40000\n",
            "    num_env_steps_sampled: 40000\n",
            "    num_env_steps_trained: 40000\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 40000\n",
            "  num_agent_steps_trained: 40000\n",
            "  num_env_steps_sampled: 40000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 40000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 88.175\n",
            "    ram_util_percent: 25.59375\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10846879655794602\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1049630942284649\n",
            "    mean_inference_ms: 1.7018725031537565\n",
            "    mean_raw_obs_processing_ms: 0.6187545799331472\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 157.94\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 157.94\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 25\n",
            "    hist_stats:\n",
            "      episode_lengths: [191, 119, 177, 200, 165, 200, 200, 190, 169, 131, 200, 185,\n",
            "        42, 135, 143, 151, 104, 177, 150, 124, 200, 200, 133, 170, 200, 200, 191, 200,\n",
            "        150, 24, 154, 118, 152, 63, 164, 159, 136, 132, 135, 133, 200, 23, 159, 200,\n",
            "        145, 96, 44, 200, 194, 150, 160, 200, 155, 136, 74, 138, 200, 200, 166, 191,\n",
            "        165, 138, 169, 111, 200, 135, 200, 200, 166, 141, 200, 170, 168, 193, 200, 200,\n",
            "        114, 51, 135, 198, 140, 74, 115, 200, 115, 189, 200, 147, 162, 200, 200, 166,\n",
            "        172, 200, 177, 169, 200, 136, 180, 200]\n",
            "      episode_reward: [191.0, 119.0, 177.0, 200.0, 165.0, 200.0, 200.0, 190.0, 169.0,\n",
            "        131.0, 200.0, 185.0, 42.0, 135.0, 143.0, 151.0, 104.0, 177.0, 150.0, 124.0,\n",
            "        200.0, 200.0, 133.0, 170.0, 200.0, 200.0, 191.0, 200.0, 150.0, 24.0, 154.0,\n",
            "        118.0, 152.0, 63.0, 164.0, 159.0, 136.0, 132.0, 135.0, 133.0, 200.0, 23.0, 159.0,\n",
            "        200.0, 145.0, 96.0, 44.0, 200.0, 194.0, 150.0, 160.0, 200.0, 155.0, 136.0, 74.0,\n",
            "        138.0, 200.0, 200.0, 166.0, 191.0, 165.0, 138.0, 169.0, 111.0, 200.0, 135.0,\n",
            "        200.0, 200.0, 166.0, 141.0, 200.0, 170.0, 168.0, 193.0, 200.0, 200.0, 114.0,\n",
            "        51.0, 135.0, 198.0, 140.0, 74.0, 115.0, 200.0, 115.0, 189.0, 200.0, 147.0, 162.0,\n",
            "        200.0, 200.0, 166.0, 172.0, 200.0, 177.0, 169.0, 200.0, 136.0, 180.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10846879655794602\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1049630942284649\n",
            "      mean_inference_ms: 1.7018725031537565\n",
            "      mean_raw_obs_processing_ms: 0.6187545799331472\n",
            "  time_since_restore: 102.36573839187622\n",
            "  time_this_iter_s: 11.243536233901978\n",
            "  time_total_s: 102.36573839187622\n",
            "  timers:\n",
            "    learn_throughput: 812.757\n",
            "    learn_time_ms: 4921.519\n",
            "    load_throughput: 7630169.183\n",
            "    load_time_ms: 0.524\n",
            "    synch_weights_time_ms: 2.197\n",
            "    training_iteration_time_ms: 10228.695\n",
            "  timestamp: 1700091073\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 40000\n",
            "  training_iteration: 10\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 44000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_agent_steps_trained: 44000\n",
            "    num_env_steps_sampled: 44000\n",
            "    num_env_steps_trained: 44000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-21\n",
            "  done: false\n",
            "  episode_len_mean: 159.44\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 159.44\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 703\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0015625000000000003\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5706532129677393\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.41240690226356186\n",
            "          kl: 0.0013648119080514624\n",
            "          policy_loss: -0.009807037791457548\n",
            "          total_loss: 9.63649650491694\n",
            "          vf_explained_var: -0.0033492983669363044\n",
            "          vf_loss: 9.646301423349689\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_agent_steps_trained: 44000\n",
            "    num_env_steps_sampled: 44000\n",
            "    num_env_steps_trained: 44000\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 44000\n",
            "  num_agent_steps_trained: 44000\n",
            "  num_env_steps_sampled: 44000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 44000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 80.62307692307692\n",
            "    ram_util_percent: 25.584615384615386\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10873310603263811\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10520454631878894\n",
            "    mean_inference_ms: 1.7061474347187655\n",
            "    mean_raw_obs_processing_ms: 0.6186152132457577\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 159.44\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 159.44\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 24\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 191, 200, 150, 24, 154, 118, 152, 63, 164, 159, 136,\n",
            "        132, 135, 133, 200, 23, 159, 200, 145, 96, 44, 200, 194, 150, 160, 200, 155,\n",
            "        136, 74, 138, 200, 200, 166, 191, 165, 138, 169, 111, 200, 135, 200, 200, 166,\n",
            "        141, 200, 170, 168, 193, 200, 200, 114, 51, 135, 198, 140, 74, 115, 200, 115,\n",
            "        189, 200, 147, 162, 200, 200, 166, 172, 200, 177, 169, 200, 136, 180, 200, 180,\n",
            "        200, 200, 139, 200, 200, 169, 154, 200, 116, 135, 190, 200, 164, 200, 143, 142,\n",
            "        200, 49, 137, 200, 200, 156, 132]\n",
            "      episode_reward: [200.0, 200.0, 191.0, 200.0, 150.0, 24.0, 154.0, 118.0, 152.0,\n",
            "        63.0, 164.0, 159.0, 136.0, 132.0, 135.0, 133.0, 200.0, 23.0, 159.0, 200.0, 145.0,\n",
            "        96.0, 44.0, 200.0, 194.0, 150.0, 160.0, 200.0, 155.0, 136.0, 74.0, 138.0, 200.0,\n",
            "        200.0, 166.0, 191.0, 165.0, 138.0, 169.0, 111.0, 200.0, 135.0, 200.0, 200.0,\n",
            "        166.0, 141.0, 200.0, 170.0, 168.0, 193.0, 200.0, 200.0, 114.0, 51.0, 135.0,\n",
            "        198.0, 140.0, 74.0, 115.0, 200.0, 115.0, 189.0, 200.0, 147.0, 162.0, 200.0,\n",
            "        200.0, 166.0, 172.0, 200.0, 177.0, 169.0, 200.0, 136.0, 180.0, 200.0, 180.0,\n",
            "        200.0, 200.0, 139.0, 200.0, 200.0, 169.0, 154.0, 200.0, 116.0, 135.0, 190.0,\n",
            "        200.0, 164.0, 200.0, 143.0, 142.0, 200.0, 49.0, 137.0, 200.0, 200.0, 156.0,\n",
            "        132.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10873310603263811\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10520454631878894\n",
            "      mean_inference_ms: 1.7061474347187655\n",
            "      mean_raw_obs_processing_ms: 0.6186152132457577\n",
            "  time_since_restore: 110.87485074996948\n",
            "  time_this_iter_s: 8.509112358093262\n",
            "  time_total_s: 110.87485074996948\n",
            "  timers:\n",
            "    learn_throughput: 810.76\n",
            "    learn_time_ms: 4933.642\n",
            "    load_throughput: 7659430.241\n",
            "    load_time_ms: 0.522\n",
            "    synch_weights_time_ms: 2.25\n",
            "    training_iteration_time_ms: 9907.863\n",
            "  timestamp: 1700091081\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 44000\n",
            "  training_iteration: 11\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 48000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_agent_steps_trained: 48000\n",
            "    num_env_steps_sampled: 48000\n",
            "    num_env_steps_trained: 48000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-32\n",
            "  done: false\n",
            "  episode_len_mean: 168.7\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 168.7\n",
            "  episode_reward_min: 39.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 726\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0007812500000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5772873069009473\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.4132401154086154\n",
            "          kl: 5.1793215197597355e-05\n",
            "          policy_loss: -0.012066371126040336\n",
            "          total_loss: 9.654093637774068\n",
            "          vf_explained_var: -0.0014207807920312368\n",
            "          vf_loss: 9.66616004513156\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_agent_steps_trained: 48000\n",
            "    num_env_steps_sampled: 48000\n",
            "    num_env_steps_trained: 48000\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 48000\n",
            "  num_agent_steps_trained: 48000\n",
            "  num_env_steps_sampled: 48000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 48000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 83.90000000000002\n",
            "    ram_util_percent: 25.60000000000001\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10892778028592394\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10530208321509793\n",
            "    mean_inference_ms: 1.7102901982218253\n",
            "    mean_raw_obs_processing_ms: 0.6176461979543955\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 168.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 168.7\n",
            "    episode_reward_min: 39.0\n",
            "    episodes_this_iter: 23\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 194, 150, 160, 200, 155, 136, 74, 138, 200, 200, 166, 191,\n",
            "        165, 138, 169, 111, 200, 135, 200, 200, 166, 141, 200, 170, 168, 193, 200, 200,\n",
            "        114, 51, 135, 198, 140, 74, 115, 200, 115, 189, 200, 147, 162, 200, 200, 166,\n",
            "        172, 200, 177, 169, 200, 136, 180, 200, 180, 200, 200, 139, 200, 200, 169, 154,\n",
            "        200, 116, 135, 190, 200, 164, 200, 143, 142, 200, 49, 137, 200, 200, 156, 132,\n",
            "        155, 200, 151, 178, 130, 118, 200, 200, 200, 195, 200, 200, 200, 39, 200, 189,\n",
            "        200, 178, 200, 171, 200, 200, 200]\n",
            "      episode_reward: [200.0, 194.0, 150.0, 160.0, 200.0, 155.0, 136.0, 74.0, 138.0,\n",
            "        200.0, 200.0, 166.0, 191.0, 165.0, 138.0, 169.0, 111.0, 200.0, 135.0, 200.0,\n",
            "        200.0, 166.0, 141.0, 200.0, 170.0, 168.0, 193.0, 200.0, 200.0, 114.0, 51.0,\n",
            "        135.0, 198.0, 140.0, 74.0, 115.0, 200.0, 115.0, 189.0, 200.0, 147.0, 162.0,\n",
            "        200.0, 200.0, 166.0, 172.0, 200.0, 177.0, 169.0, 200.0, 136.0, 180.0, 200.0,\n",
            "        180.0, 200.0, 200.0, 139.0, 200.0, 200.0, 169.0, 154.0, 200.0, 116.0, 135.0,\n",
            "        190.0, 200.0, 164.0, 200.0, 143.0, 142.0, 200.0, 49.0, 137.0, 200.0, 200.0,\n",
            "        156.0, 132.0, 155.0, 200.0, 151.0, 178.0, 130.0, 118.0, 200.0, 200.0, 200.0,\n",
            "        195.0, 200.0, 200.0, 200.0, 39.0, 200.0, 189.0, 200.0, 178.0, 200.0, 171.0,\n",
            "        200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10892778028592394\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10530208321509793\n",
            "      mean_inference_ms: 1.7102901982218253\n",
            "      mean_raw_obs_processing_ms: 0.6176461979543955\n",
            "  time_since_restore: 121.61359739303589\n",
            "  time_this_iter_s: 10.738746643066406\n",
            "  time_total_s: 121.61359739303589\n",
            "  timers:\n",
            "    learn_throughput: 821.871\n",
            "    learn_time_ms: 4866.945\n",
            "    load_throughput: 7569921.04\n",
            "    load_time_ms: 0.528\n",
            "    synch_weights_time_ms: 2.273\n",
            "    training_iteration_time_ms: 10116.275\n",
            "  timestamp: 1700091092\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 48000\n",
            "  training_iteration: 12\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 52000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_agent_steps_trained: 52000\n",
            "    num_env_steps_sampled: 52000\n",
            "    num_env_steps_trained: 52000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-41\n",
            "  done: false\n",
            "  episode_len_mean: 170.87\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 170.87\n",
            "  episode_reward_min: 39.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 748\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0003906250000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5838612649389493\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.45231474451320147\n",
            "          kl: 0.0060271410131537\n",
            "          policy_loss: -0.014463472464472375\n",
            "          total_loss: 9.620299741273286\n",
            "          vf_explained_var: -0.0007774061413221462\n",
            "          vf_loss: 9.634760830992011\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_agent_steps_trained: 52000\n",
            "    num_env_steps_sampled: 52000\n",
            "    num_env_steps_trained: 52000\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 52000\n",
            "  num_agent_steps_trained: 52000\n",
            "  num_env_steps_sampled: 52000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 52000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 90.49230769230766\n",
            "    ram_util_percent: 25.600000000000005\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10913465377263538\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10547986193554909\n",
            "    mean_inference_ms: 1.7148872462262075\n",
            "    mean_raw_obs_processing_ms: 0.6172635130556111\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 170.87\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 170.87\n",
            "    episode_reward_min: 39.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [141, 200, 170, 168, 193, 200, 200, 114, 51, 135, 198, 140, 74,\n",
            "        115, 200, 115, 189, 200, 147, 162, 200, 200, 166, 172, 200, 177, 169, 200, 136,\n",
            "        180, 200, 180, 200, 200, 139, 200, 200, 169, 154, 200, 116, 135, 190, 200, 164,\n",
            "        200, 143, 142, 200, 49, 137, 200, 200, 156, 132, 155, 200, 151, 178, 130, 118,\n",
            "        200, 200, 200, 195, 200, 200, 200, 39, 200, 189, 200, 178, 200, 171, 200, 200,\n",
            "        200, 158, 192, 200, 128, 200, 200, 165, 176, 156, 147, 178, 191, 200, 105, 170,\n",
            "        154, 160, 200, 200, 200, 200, 185]\n",
            "      episode_reward: [141.0, 200.0, 170.0, 168.0, 193.0, 200.0, 200.0, 114.0, 51.0,\n",
            "        135.0, 198.0, 140.0, 74.0, 115.0, 200.0, 115.0, 189.0, 200.0, 147.0, 162.0,\n",
            "        200.0, 200.0, 166.0, 172.0, 200.0, 177.0, 169.0, 200.0, 136.0, 180.0, 200.0,\n",
            "        180.0, 200.0, 200.0, 139.0, 200.0, 200.0, 169.0, 154.0, 200.0, 116.0, 135.0,\n",
            "        190.0, 200.0, 164.0, 200.0, 143.0, 142.0, 200.0, 49.0, 137.0, 200.0, 200.0,\n",
            "        156.0, 132.0, 155.0, 200.0, 151.0, 178.0, 130.0, 118.0, 200.0, 200.0, 200.0,\n",
            "        195.0, 200.0, 200.0, 200.0, 39.0, 200.0, 189.0, 200.0, 178.0, 200.0, 171.0,\n",
            "        200.0, 200.0, 200.0, 158.0, 192.0, 200.0, 128.0, 200.0, 200.0, 165.0, 176.0,\n",
            "        156.0, 147.0, 178.0, 191.0, 200.0, 105.0, 170.0, 154.0, 160.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 185.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10913465377263538\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10547986193554909\n",
            "      mean_inference_ms: 1.7148872462262075\n",
            "      mean_raw_obs_processing_ms: 0.6172635130556111\n",
            "  time_since_restore: 130.57682752609253\n",
            "  time_this_iter_s: 8.96323013305664\n",
            "  time_total_s: 130.57682752609253\n",
            "  timers:\n",
            "    learn_throughput: 805.146\n",
            "    learn_time_ms: 4968.044\n",
            "    load_throughput: 7523415.247\n",
            "    load_time_ms: 0.532\n",
            "    synch_weights_time_ms: 2.322\n",
            "    training_iteration_time_ms: 9963.391\n",
            "  timestamp: 1700091101\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 52000\n",
            "  training_iteration: 13\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 56000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_agent_steps_trained: 56000\n",
            "    num_env_steps_sampled: 56000\n",
            "    num_env_steps_trained: 56000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-31-51\n",
            "  done: false\n",
            "  episode_len_mean: 176.89\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 176.89\n",
            "  episode_reward_min: 39.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 770\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0003906250000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5903534491215983\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.4226021288703847\n",
            "          kl: 0.0014224185904636358\n",
            "          policy_loss: -0.012558634900137462\n",
            "          total_loss: 9.651459741079679\n",
            "          vf_explained_var: -0.006228553736081687\n",
            "          vf_loss: 9.66401788239838\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_agent_steps_trained: 56000\n",
            "    num_env_steps_sampled: 56000\n",
            "    num_env_steps_trained: 56000\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 56000\n",
            "  num_agent_steps_trained: 56000\n",
            "  num_env_steps_sampled: 56000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 56000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 83.30714285714285\n",
            "    ram_util_percent: 25.578571428571433\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10937671792941465\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10554528009278728\n",
            "    mean_inference_ms: 1.718281258261256\n",
            "    mean_raw_obs_processing_ms: 0.6162506984770387\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 176.89\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 176.89\n",
            "    episode_reward_min: 39.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [166, 172, 200, 177, 169, 200, 136, 180, 200, 180, 200, 200,\n",
            "        139, 200, 200, 169, 154, 200, 116, 135, 190, 200, 164, 200, 143, 142, 200, 49,\n",
            "        137, 200, 200, 156, 132, 155, 200, 151, 178, 130, 118, 200, 200, 200, 195, 200,\n",
            "        200, 200, 39, 200, 189, 200, 178, 200, 171, 200, 200, 200, 158, 192, 200, 128,\n",
            "        200, 200, 165, 176, 156, 147, 178, 191, 200, 105, 170, 154, 160, 200, 200, 200,\n",
            "        200, 185, 200, 166, 200, 200, 200, 200, 143, 200, 170, 200, 200, 200, 200, 200,\n",
            "        200, 199, 200, 164, 200, 200, 173, 99]\n",
            "      episode_reward: [166.0, 172.0, 200.0, 177.0, 169.0, 200.0, 136.0, 180.0, 200.0,\n",
            "        180.0, 200.0, 200.0, 139.0, 200.0, 200.0, 169.0, 154.0, 200.0, 116.0, 135.0,\n",
            "        190.0, 200.0, 164.0, 200.0, 143.0, 142.0, 200.0, 49.0, 137.0, 200.0, 200.0,\n",
            "        156.0, 132.0, 155.0, 200.0, 151.0, 178.0, 130.0, 118.0, 200.0, 200.0, 200.0,\n",
            "        195.0, 200.0, 200.0, 200.0, 39.0, 200.0, 189.0, 200.0, 178.0, 200.0, 171.0,\n",
            "        200.0, 200.0, 200.0, 158.0, 192.0, 200.0, 128.0, 200.0, 200.0, 165.0, 176.0,\n",
            "        156.0, 147.0, 178.0, 191.0, 200.0, 105.0, 170.0, 154.0, 160.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 185.0, 200.0, 166.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0,\n",
            "        170.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 199.0, 200.0, 164.0, 200.0,\n",
            "        200.0, 173.0, 99.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10937671792941465\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10554528009278728\n",
            "      mean_inference_ms: 1.718281258261256\n",
            "      mean_raw_obs_processing_ms: 0.6162506984770387\n",
            "  time_since_restore: 140.6848726272583\n",
            "  time_this_iter_s: 10.108045101165771\n",
            "  time_total_s: 140.6848726272583\n",
            "  timers:\n",
            "    learn_throughput: 910.398\n",
            "    learn_time_ms: 4393.683\n",
            "    load_throughput: 7645468.465\n",
            "    load_time_ms: 0.523\n",
            "    synch_weights_time_ms: 2.278\n",
            "    training_iteration_time_ms: 9595.58\n",
            "  timestamp: 1700091111\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 56000\n",
            "  training_iteration: 14\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 60000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_agent_steps_trained: 60000\n",
            "    num_env_steps_sampled: 60000\n",
            "    num_env_steps_trained: 60000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-01\n",
            "  done: false\n",
            "  episode_len_mean: 178.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 178.45\n",
            "  episode_reward_min: 39.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 792\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.00019531250000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5866235317722444\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.450668698132679\n",
            "          kl: 0.00041776370551327267\n",
            "          policy_loss: -0.009616693050190006\n",
            "          total_loss: 9.638305128774336\n",
            "          vf_explained_var: -0.015384585754845732\n",
            "          vf_loss: 9.647921808304325\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_agent_steps_trained: 60000\n",
            "    num_env_steps_sampled: 60000\n",
            "    num_env_steps_trained: 60000\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 60000\n",
            "  num_agent_steps_trained: 60000\n",
            "  num_env_steps_sampled: 60000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 60000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 95.10714285714285\n",
            "    ram_util_percent: 25.514285714285716\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10953120715804522\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10549243255271787\n",
            "    mean_inference_ms: 1.7186963206029278\n",
            "    mean_raw_obs_processing_ms: 0.6139636056711459\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 178.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 178.45\n",
            "    episode_reward_min: 39.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [164, 200, 143, 142, 200, 49, 137, 200, 200, 156, 132, 155, 200,\n",
            "        151, 178, 130, 118, 200, 200, 200, 195, 200, 200, 200, 39, 200, 189, 200, 178,\n",
            "        200, 171, 200, 200, 200, 158, 192, 200, 128, 200, 200, 165, 176, 156, 147, 178,\n",
            "        191, 200, 105, 170, 154, 160, 200, 200, 200, 200, 185, 200, 166, 200, 200, 200,\n",
            "        200, 143, 200, 170, 200, 200, 200, 200, 200, 200, 199, 200, 164, 200, 200, 173,\n",
            "        99, 200, 200, 200, 200, 200, 148, 200, 182, 135, 200, 200, 166, 159, 171, 200,\n",
            "        200, 200, 200, 200, 200, 200, 78]\n",
            "      episode_reward: [164.0, 200.0, 143.0, 142.0, 200.0, 49.0, 137.0, 200.0, 200.0,\n",
            "        156.0, 132.0, 155.0, 200.0, 151.0, 178.0, 130.0, 118.0, 200.0, 200.0, 200.0,\n",
            "        195.0, 200.0, 200.0, 200.0, 39.0, 200.0, 189.0, 200.0, 178.0, 200.0, 171.0,\n",
            "        200.0, 200.0, 200.0, 158.0, 192.0, 200.0, 128.0, 200.0, 200.0, 165.0, 176.0,\n",
            "        156.0, 147.0, 178.0, 191.0, 200.0, 105.0, 170.0, 154.0, 160.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 185.0, 200.0, 166.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0,\n",
            "        170.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 199.0, 200.0, 164.0, 200.0,\n",
            "        200.0, 173.0, 99.0, 200.0, 200.0, 200.0, 200.0, 200.0, 148.0, 200.0, 182.0,\n",
            "        135.0, 200.0, 200.0, 166.0, 159.0, 171.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 78.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10953120715804522\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10549243255271787\n",
            "      mean_inference_ms: 1.7186963206029278\n",
            "      mean_raw_obs_processing_ms: 0.6139636056711459\n",
            "  time_since_restore: 150.5670404434204\n",
            "  time_this_iter_s: 9.88216781616211\n",
            "  time_total_s: 150.5670404434204\n",
            "  timers:\n",
            "    learn_throughput: 875.302\n",
            "    learn_time_ms: 4569.854\n",
            "    load_throughput: 7342969.188\n",
            "    load_time_ms: 0.545\n",
            "    synch_weights_time_ms: 2.688\n",
            "    training_iteration_time_ms: 9766.864\n",
            "  timestamp: 1700091121\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 60000\n",
            "  training_iteration: 15\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 64000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_agent_steps_trained: 64000\n",
            "    num_env_steps_sampled: 64000\n",
            "    num_env_steps_trained: 64000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-11\n",
            "  done: false\n",
            "  episode_len_mean: 181.78\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 181.78\n",
            "  episode_reward_min: 39.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 814\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.765625000000002e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5876299842070508\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.3378867341425791\n",
            "          kl: 0.0004660402708999864\n",
            "          policy_loss: -0.009118134042708784\n",
            "          total_loss: 9.647375984602077\n",
            "          vf_explained_var: -0.0039149133107995475\n",
            "          vf_loss: 9.65649409653038\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_agent_steps_trained: 64000\n",
            "    num_env_steps_sampled: 64000\n",
            "    num_env_steps_trained: 64000\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 64000\n",
            "  num_agent_steps_trained: 64000\n",
            "  num_env_steps_sampled: 64000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 64000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 82.31428571428569\n",
            "    ram_util_percent: 25.371428571428567\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10962871100385399\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10563496404223972\n",
            "    mean_inference_ms: 1.719739884817783\n",
            "    mean_raw_obs_processing_ms: 0.611856563553472\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 181.78\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 181.78\n",
            "    episode_reward_min: 39.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 39, 200, 189, 200, 178, 200, 171, 200, 200, 200, 158,\n",
            "        192, 200, 128, 200, 200, 165, 176, 156, 147, 178, 191, 200, 105, 170, 154, 160,\n",
            "        200, 200, 200, 200, 185, 200, 166, 200, 200, 200, 200, 143, 200, 170, 200, 200,\n",
            "        200, 200, 200, 200, 199, 200, 164, 200, 200, 173, 99, 200, 200, 200, 200, 200,\n",
            "        148, 200, 182, 135, 200, 200, 166, 159, 171, 200, 200, 200, 200, 200, 200, 200,\n",
            "        78, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 91, 200, 174, 200, 200,\n",
            "        95, 200, 137, 200, 200, 86, 200]\n",
            "      episode_reward: [200.0, 200.0, 39.0, 200.0, 189.0, 200.0, 178.0, 200.0, 171.0,\n",
            "        200.0, 200.0, 200.0, 158.0, 192.0, 200.0, 128.0, 200.0, 200.0, 165.0, 176.0,\n",
            "        156.0, 147.0, 178.0, 191.0, 200.0, 105.0, 170.0, 154.0, 160.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 185.0, 200.0, 166.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0,\n",
            "        170.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 199.0, 200.0, 164.0, 200.0,\n",
            "        200.0, 173.0, 99.0, 200.0, 200.0, 200.0, 200.0, 200.0, 148.0, 200.0, 182.0,\n",
            "        135.0, 200.0, 200.0, 166.0, 159.0, 171.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 78.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 91.0, 200.0, 174.0, 200.0, 200.0, 95.0, 200.0, 137.0, 200.0, 200.0,\n",
            "        86.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10962871100385399\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10563496404223972\n",
            "      mean_inference_ms: 1.719739884817783\n",
            "      mean_raw_obs_processing_ms: 0.611856563553472\n",
            "  time_since_restore: 160.09631037712097\n",
            "  time_this_iter_s: 9.529269933700562\n",
            "  time_total_s: 160.09631037712097\n",
            "  timers:\n",
            "    learn_throughput: 904.134\n",
            "    learn_time_ms: 4424.121\n",
            "    load_throughput: 7683282.653\n",
            "    load_time_ms: 0.521\n",
            "    synch_weights_time_ms: 2.681\n",
            "    training_iteration_time_ms: 9614.147\n",
            "  timestamp: 1700091131\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 64000\n",
            "  training_iteration: 16\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 68000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_agent_steps_trained: 68000\n",
            "    num_env_steps_sampled: 68000\n",
            "    num_env_steps_trained: 68000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-20\n",
            "  done: false\n",
            "  episode_len_mean: 181.54\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 181.54\n",
            "  episode_reward_min: 78.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 836\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.882812500000001e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5763944038780787\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.435748347351628\n",
            "          kl: 0.0032907501201904425\n",
            "          policy_loss: -0.01191231187192663\n",
            "          total_loss: 9.6092087335484\n",
            "          vf_explained_var: -0.0033434744804136216\n",
            "          vf_loss: 9.6211209348453\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_agent_steps_trained: 68000\n",
            "    num_env_steps_sampled: 68000\n",
            "    num_env_steps_trained: 68000\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 68000\n",
            "  num_agent_steps_trained: 68000\n",
            "  num_env_steps_sampled: 68000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 68000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 98.1642857142857\n",
            "    ram_util_percent: 25.399999999999995\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10926992951701689\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10536694721507059\n",
            "    mean_inference_ms: 1.7143716965083806\n",
            "    mean_raw_obs_processing_ms: 0.6085871787037427\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 181.54\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 181.54\n",
            "    episode_reward_min: 78.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [178, 191, 200, 105, 170, 154, 160, 200, 200, 200, 200, 185,\n",
            "        200, 166, 200, 200, 200, 200, 143, 200, 170, 200, 200, 200, 200, 200, 200, 199,\n",
            "        200, 164, 200, 200, 173, 99, 200, 200, 200, 200, 200, 148, 200, 182, 135, 200,\n",
            "        200, 166, 159, 171, 200, 200, 200, 200, 200, 200, 200, 78, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 91, 200, 174, 200, 200, 95, 200, 137, 200, 200,\n",
            "        86, 200, 93, 200, 200, 127, 200, 200, 200, 200, 200, 200, 169, 200, 200, 200,\n",
            "        200, 116, 199, 90, 81, 200, 200, 200]\n",
            "      episode_reward: [178.0, 191.0, 200.0, 105.0, 170.0, 154.0, 160.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 185.0, 200.0, 166.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0,\n",
            "        170.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 199.0, 200.0, 164.0, 200.0,\n",
            "        200.0, 173.0, 99.0, 200.0, 200.0, 200.0, 200.0, 200.0, 148.0, 200.0, 182.0,\n",
            "        135.0, 200.0, 200.0, 166.0, 159.0, 171.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 78.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 91.0, 200.0, 174.0, 200.0, 200.0, 95.0, 200.0, 137.0, 200.0, 200.0,\n",
            "        86.0, 200.0, 93.0, 200.0, 200.0, 127.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        169.0, 200.0, 200.0, 200.0, 200.0, 116.0, 199.0, 90.0, 81.0, 200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10926992951701689\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10536694721507059\n",
            "      mean_inference_ms: 1.7143716965083806\n",
            "      mean_raw_obs_processing_ms: 0.6085871787037427\n",
            "  time_since_restore: 169.79556274414062\n",
            "  time_this_iter_s: 9.699252367019653\n",
            "  time_total_s: 169.79556274414062\n",
            "  timers:\n",
            "    learn_throughput: 876.886\n",
            "    learn_time_ms: 4561.597\n",
            "    load_throughput: 7653141.137\n",
            "    load_time_ms: 0.523\n",
            "    synch_weights_time_ms: 2.765\n",
            "    training_iteration_time_ms: 9773.78\n",
            "  timestamp: 1700091140\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 68000\n",
            "  training_iteration: 17\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 72000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_agent_steps_trained: 72000\n",
            "    num_env_steps_sampled: 72000\n",
            "    num_env_steps_trained: 72000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-29\n",
            "  done: false\n",
            "  episode_len_mean: 182.47\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 182.47\n",
            "  episode_reward_min: 78.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 857\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.4414062500000005e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5853858988131246\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5090545687864545\n",
            "          kl: 0.0023366635586663975\n",
            "          policy_loss: -0.009314415561816385\n",
            "          total_loss: 9.618197385726436\n",
            "          vf_explained_var: -0.007532161666500953\n",
            "          vf_loss: 9.627511796643658\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_agent_steps_trained: 72000\n",
            "    num_env_steps_sampled: 72000\n",
            "    num_env_steps_trained: 72000\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 72000\n",
            "  num_agent_steps_trained: 72000\n",
            "  num_env_steps_sampled: 72000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 72000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 81.61666666666667\n",
            "    ram_util_percent: 25.325000000000003\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.108996516579763\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10505173737706584\n",
            "    mean_inference_ms: 1.7098427994241132\n",
            "    mean_raw_obs_processing_ms: 0.6057430920593343\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 182.47\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 182.47\n",
            "    episode_reward_min: 78.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 200, 200, 200, 199, 200, 164, 200, 200, 173,\n",
            "        99, 200, 200, 200, 200, 200, 148, 200, 182, 135, 200, 200, 166, 159, 171, 200,\n",
            "        200, 200, 200, 200, 200, 200, 78, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 91, 200, 174, 200, 200, 95, 200, 137, 200, 200, 86, 200, 93, 200, 200,\n",
            "        127, 200, 200, 200, 200, 200, 200, 169, 200, 200, 200, 200, 116, 199, 90, 81,\n",
            "        200, 200, 200, 200, 161, 200, 200, 171, 196, 200, 200, 200, 200, 200, 152, 200,\n",
            "        181, 200, 155, 175, 180, 200, 144, 200]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 199.0, 200.0, 164.0,\n",
            "        200.0, 200.0, 173.0, 99.0, 200.0, 200.0, 200.0, 200.0, 200.0, 148.0, 200.0,\n",
            "        182.0, 135.0, 200.0, 200.0, 166.0, 159.0, 171.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 78.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 91.0, 200.0, 174.0, 200.0, 200.0, 95.0, 200.0, 137.0, 200.0,\n",
            "        200.0, 86.0, 200.0, 93.0, 200.0, 200.0, 127.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 169.0, 200.0, 200.0, 200.0, 200.0, 116.0, 199.0, 90.0, 81.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 161.0, 200.0, 200.0, 171.0, 196.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 152.0, 200.0, 181.0, 200.0, 155.0, 175.0, 180.0, 200.0, 144.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.108996516579763\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10505173737706584\n",
            "      mean_inference_ms: 1.7098427994241132\n",
            "      mean_raw_obs_processing_ms: 0.6057430920593343\n",
            "  time_since_restore: 178.52031922340393\n",
            "  time_this_iter_s: 8.724756479263306\n",
            "  time_total_s: 178.52031922340393\n",
            "  timers:\n",
            "    learn_throughput: 900.47\n",
            "    learn_time_ms: 4442.127\n",
            "    load_throughput: 7950156.85\n",
            "    load_time_ms: 0.503\n",
            "    synch_weights_time_ms: 2.695\n",
            "    training_iteration_time_ms: 9530.413\n",
            "  timestamp: 1700091149\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 72000\n",
            "  training_iteration: 18\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 76000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_agent_steps_trained: 76000\n",
            "    num_env_steps_sampled: 76000\n",
            "    num_env_steps_trained: 76000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-39\n",
            "  done: false\n",
            "  episode_len_mean: 178.88\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 178.88\n",
            "  episode_reward_min: 78.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 881\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.2207031250000002e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5907242880072645\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5401850248376528\n",
            "          kl: 0.0038803430489721363\n",
            "          policy_loss: -0.013420406280345814\n",
            "          total_loss: 9.597843346544492\n",
            "          vf_explained_var: 0.012112946407769316\n",
            "          vf_loss: 9.611263783772786\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_agent_steps_trained: 76000\n",
            "    num_env_steps_sampled: 76000\n",
            "    num_env_steps_trained: 76000\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 76000\n",
            "  num_agent_steps_trained: 76000\n",
            "  num_env_steps_sampled: 76000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 76000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 97.27142857142857\n",
            "    ram_util_percent: 25.285714285714285\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1082829945853485\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10430773330785781\n",
            "    mean_inference_ms: 1.7013928830115563\n",
            "    mean_raw_obs_processing_ms: 0.6016417806641533\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 178.88\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 178.88\n",
            "    episode_reward_min: 78.0\n",
            "    episodes_this_iter: 24\n",
            "    hist_stats:\n",
            "      episode_lengths: [166, 159, 171, 200, 200, 200, 200, 200, 200, 200, 78, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 91, 200, 174, 200, 200, 95, 200, 137,\n",
            "        200, 200, 86, 200, 93, 200, 200, 127, 200, 200, 200, 200, 200, 200, 169, 200,\n",
            "        200, 200, 200, 116, 199, 90, 81, 200, 200, 200, 200, 161, 200, 200, 171, 196,\n",
            "        200, 200, 200, 200, 200, 152, 200, 181, 200, 155, 175, 180, 200, 144, 200, 194,\n",
            "        200, 200, 183, 158, 200, 156, 200, 146, 145, 200, 200, 153, 125, 200, 200, 149,\n",
            "        125, 154, 200, 147, 152, 154, 200]\n",
            "      episode_reward: [166.0, 159.0, 171.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 78.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 91.0, 200.0, 174.0, 200.0, 200.0, 95.0, 200.0, 137.0, 200.0, 200.0, 86.0,\n",
            "        200.0, 93.0, 200.0, 200.0, 127.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        169.0, 200.0, 200.0, 200.0, 200.0, 116.0, 199.0, 90.0, 81.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 161.0, 200.0, 200.0, 171.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        152.0, 200.0, 181.0, 200.0, 155.0, 175.0, 180.0, 200.0, 144.0, 200.0, 194.0,\n",
            "        200.0, 200.0, 183.0, 158.0, 200.0, 156.0, 200.0, 146.0, 145.0, 200.0, 200.0,\n",
            "        153.0, 125.0, 200.0, 200.0, 149.0, 125.0, 154.0, 200.0, 147.0, 152.0, 154.0,\n",
            "        200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1082829945853485\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10430773330785781\n",
            "      mean_inference_ms: 1.7013928830115563\n",
            "      mean_raw_obs_processing_ms: 0.6016417806641533\n",
            "  time_since_restore: 188.38089203834534\n",
            "  time_this_iter_s: 9.860572814941406\n",
            "  time_total_s: 188.38089203834534\n",
            "  timers:\n",
            "    learn_throughput: 871.586\n",
            "    learn_time_ms: 4589.337\n",
            "    load_throughput: 7480811.522\n",
            "    load_time_ms: 0.535\n",
            "    synch_weights_time_ms: 3.048\n",
            "    training_iteration_time_ms: 9717.08\n",
            "  timestamp: 1700091159\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 76000\n",
            "  training_iteration: 19\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 80000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_agent_steps_trained: 80000\n",
            "    num_env_steps_sampled: 80000\n",
            "    num_env_steps_trained: 80000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-47\n",
            "  done: false\n",
            "  episode_len_mean: 177.5\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 177.5\n",
            "  episode_reward_min: 81.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 903\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.103515625000001e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.584484447715103\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6663979467124709\n",
            "          kl: 0.0010744536794652763\n",
            "          policy_loss: -0.008429489392907389\n",
            "          total_loss: 9.639043078884002\n",
            "          vf_explained_var: -0.004368222657070365\n",
            "          vf_loss: 9.647472537461148\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_agent_steps_trained: 80000\n",
            "    num_env_steps_sampled: 80000\n",
            "    num_env_steps_trained: 80000\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 80000\n",
            "  num_agent_steps_trained: 80000\n",
            "  num_env_steps_sampled: 80000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 80000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 81.70833333333334\n",
            "    ram_util_percent: 25.291666666666668\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10780240519044472\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10370802835174243\n",
            "    mean_inference_ms: 1.6939619233251357\n",
            "    mean_raw_obs_processing_ms: 0.5980521206730408\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 177.5\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 177.5\n",
            "    episode_reward_min: 81.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 174, 200, 200, 95, 200, 137, 200, 200, 86, 200, 93, 200,\n",
            "        200, 127, 200, 200, 200, 200, 200, 200, 169, 200, 200, 200, 200, 116, 199, 90,\n",
            "        81, 200, 200, 200, 200, 161, 200, 200, 171, 196, 200, 200, 200, 200, 200, 152,\n",
            "        200, 181, 200, 155, 175, 180, 200, 144, 200, 194, 200, 200, 183, 158, 200, 156,\n",
            "        200, 146, 145, 200, 200, 153, 125, 200, 200, 149, 125, 154, 200, 147, 152, 154,\n",
            "        200, 200, 162, 200, 186, 159, 157, 169, 200, 200, 200, 200, 149, 160, 134, 176,\n",
            "        200, 200, 186, 173, 200, 181, 135]\n",
            "      episode_reward: [200.0, 174.0, 200.0, 200.0, 95.0, 200.0, 137.0, 200.0, 200.0,\n",
            "        86.0, 200.0, 93.0, 200.0, 200.0, 127.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        169.0, 200.0, 200.0, 200.0, 200.0, 116.0, 199.0, 90.0, 81.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 161.0, 200.0, 200.0, 171.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        152.0, 200.0, 181.0, 200.0, 155.0, 175.0, 180.0, 200.0, 144.0, 200.0, 194.0,\n",
            "        200.0, 200.0, 183.0, 158.0, 200.0, 156.0, 200.0, 146.0, 145.0, 200.0, 200.0,\n",
            "        153.0, 125.0, 200.0, 200.0, 149.0, 125.0, 154.0, 200.0, 147.0, 152.0, 154.0,\n",
            "        200.0, 200.0, 162.0, 200.0, 186.0, 159.0, 157.0, 169.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 149.0, 160.0, 134.0, 176.0, 200.0, 200.0, 186.0, 173.0, 200.0, 181.0,\n",
            "        135.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10780240519044472\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10370802835174243\n",
            "      mean_inference_ms: 1.6939619233251357\n",
            "      mean_raw_obs_processing_ms: 0.5980521206730408\n",
            "  time_since_restore: 196.80210161209106\n",
            "  time_this_iter_s: 8.421209573745728\n",
            "  time_total_s: 196.80210161209106\n",
            "  timers:\n",
            "    learn_throughput: 883.813\n",
            "    learn_time_ms: 4525.845\n",
            "    load_throughput: 7872561.588\n",
            "    load_time_ms: 0.508\n",
            "    synch_weights_time_ms: 3.083\n",
            "    training_iteration_time_ms: 9435.107\n",
            "  timestamp: 1700091167\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 80000\n",
            "  training_iteration: 20\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 84000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_agent_steps_trained: 84000\n",
            "    num_env_steps_sampled: 84000\n",
            "    num_env_steps_trained: 84000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-32-58\n",
            "  done: false\n",
            "  episode_len_mean: 180.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 180.36\n",
            "  episode_reward_min: 81.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 924\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.0517578125000006e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6007187893313747\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6006812985265447\n",
            "          kl: 0.002594706538719807\n",
            "          policy_loss: -0.009120317206527758\n",
            "          total_loss: 9.649327653454195\n",
            "          vf_explained_var: 0.0010823217130476429\n",
            "          vf_loss: 9.658447959858885\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_agent_steps_trained: 84000\n",
            "    num_env_steps_sampled: 84000\n",
            "    num_env_steps_trained: 84000\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 84000\n",
            "  num_agent_steps_trained: 84000\n",
            "  num_env_steps_sampled: 84000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 84000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 97.5125\n",
            "    ram_util_percent: 25.225\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10728853392294656\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10310704297991072\n",
            "    mean_inference_ms: 1.686870774316327\n",
            "    mean_raw_obs_processing_ms: 0.5948779921364334\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 180.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 180.36\n",
            "    episode_reward_min: 81.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [169, 200, 200, 200, 200, 116, 199, 90, 81, 200, 200, 200, 200,\n",
            "        161, 200, 200, 171, 196, 200, 200, 200, 200, 200, 152, 200, 181, 200, 155, 175,\n",
            "        180, 200, 144, 200, 194, 200, 200, 183, 158, 200, 156, 200, 146, 145, 200, 200,\n",
            "        153, 125, 200, 200, 149, 125, 154, 200, 147, 152, 154, 200, 200, 162, 200, 186,\n",
            "        159, 157, 169, 200, 200, 200, 200, 149, 160, 134, 176, 200, 200, 186, 173, 200,\n",
            "        181, 135, 200, 200, 200, 159, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        172, 200, 156, 180, 178, 153, 200]\n",
            "      episode_reward: [169.0, 200.0, 200.0, 200.0, 200.0, 116.0, 199.0, 90.0, 81.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 161.0, 200.0, 200.0, 171.0, 196.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 152.0, 200.0, 181.0, 200.0, 155.0, 175.0, 180.0, 200.0,\n",
            "        144.0, 200.0, 194.0, 200.0, 200.0, 183.0, 158.0, 200.0, 156.0, 200.0, 146.0,\n",
            "        145.0, 200.0, 200.0, 153.0, 125.0, 200.0, 200.0, 149.0, 125.0, 154.0, 200.0,\n",
            "        147.0, 152.0, 154.0, 200.0, 200.0, 162.0, 200.0, 186.0, 159.0, 157.0, 169.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 149.0, 160.0, 134.0, 176.0, 200.0, 200.0, 186.0,\n",
            "        173.0, 200.0, 181.0, 135.0, 200.0, 200.0, 200.0, 159.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 172.0, 200.0, 156.0, 180.0,\n",
            "        178.0, 153.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10728853392294656\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10310704297991072\n",
            "      mean_inference_ms: 1.686870774316327\n",
            "      mean_raw_obs_processing_ms: 0.5948779921364334\n",
            "  time_since_restore: 207.6056900024414\n",
            "  time_this_iter_s: 10.803588390350342\n",
            "  time_total_s: 207.6056900024414\n",
            "  timers:\n",
            "    learn_throughput: 856.255\n",
            "    learn_time_ms: 4671.506\n",
            "    load_throughput: 7519368.949\n",
            "    load_time_ms: 0.532\n",
            "    synch_weights_time_ms: 3.035\n",
            "    training_iteration_time_ms: 9665.345\n",
            "  timestamp: 1700091178\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 84000\n",
            "  training_iteration: 21\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 88000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_agent_steps_trained: 88000\n",
            "    num_env_steps_sampled: 88000\n",
            "    num_env_steps_trained: 88000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-06\n",
            "  done: false\n",
            "  episode_len_mean: 176.67\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 176.67\n",
            "  episode_reward_min: 62.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 949\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.5258789062500003e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5782342923584805\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5713447920337159\n",
            "          kl: 0.0026543226709636373\n",
            "          policy_loss: -0.01409217529599705\n",
            "          total_loss: 9.563934726099815\n",
            "          vf_explained_var: -0.02597753277388952\n",
            "          vf_loss: 9.578026882294685\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_agent_steps_trained: 88000\n",
            "    num_env_steps_sampled: 88000\n",
            "    num_env_steps_trained: 88000\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 88000\n",
            "  num_agent_steps_trained: 88000\n",
            "  num_env_steps_sampled: 88000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 88000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 78.56363636363636\n",
            "    ram_util_percent: 25.16363636363636\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10669659011026783\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10239241033742154\n",
            "    mean_inference_ms: 1.6771079962126572\n",
            "    mean_raw_obs_processing_ms: 0.5911688327076405\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 176.67\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 176.67\n",
            "    episode_reward_min: 62.0\n",
            "    episodes_this_iter: 25\n",
            "    hist_stats:\n",
            "      episode_lengths: [181, 200, 155, 175, 180, 200, 144, 200, 194, 200, 200, 183,\n",
            "        158, 200, 156, 200, 146, 145, 200, 200, 153, 125, 200, 200, 149, 125, 154, 200,\n",
            "        147, 152, 154, 200, 200, 162, 200, 186, 159, 157, 169, 200, 200, 200, 200, 149,\n",
            "        160, 134, 176, 200, 200, 186, 173, 200, 181, 135, 200, 200, 200, 159, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 172, 200, 156, 180, 178, 153, 200, 187,\n",
            "        126, 62, 200, 117, 196, 200, 200, 200, 200, 200, 200, 200, 93, 200, 200, 67,\n",
            "        162, 169, 115, 72, 200, 200, 200, 200]\n",
            "      episode_reward: [181.0, 200.0, 155.0, 175.0, 180.0, 200.0, 144.0, 200.0, 194.0,\n",
            "        200.0, 200.0, 183.0, 158.0, 200.0, 156.0, 200.0, 146.0, 145.0, 200.0, 200.0,\n",
            "        153.0, 125.0, 200.0, 200.0, 149.0, 125.0, 154.0, 200.0, 147.0, 152.0, 154.0,\n",
            "        200.0, 200.0, 162.0, 200.0, 186.0, 159.0, 157.0, 169.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 149.0, 160.0, 134.0, 176.0, 200.0, 200.0, 186.0, 173.0, 200.0, 181.0,\n",
            "        135.0, 200.0, 200.0, 200.0, 159.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 172.0, 200.0, 156.0, 180.0, 178.0, 153.0, 200.0,\n",
            "        187.0, 126.0, 62.0, 200.0, 117.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 93.0, 200.0, 200.0, 67.0, 162.0, 169.0, 115.0, 72.0, 200.0, 200.0,\n",
            "        200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10669659011026783\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10239241033742154\n",
            "      mean_inference_ms: 1.6771079962126572\n",
            "      mean_raw_obs_processing_ms: 0.5911688327076405\n",
            "  time_since_restore: 215.59053111076355\n",
            "  time_this_iter_s: 7.9848411083221436\n",
            "  time_total_s: 215.59053111076355\n",
            "  timers:\n",
            "    learn_throughput: 857.497\n",
            "    learn_time_ms: 4664.741\n",
            "    load_throughput: 7472148.933\n",
            "    load_time_ms: 0.535\n",
            "    synch_weights_time_ms: 3.04\n",
            "    training_iteration_time_ms: 9389.907\n",
            "  timestamp: 1700091186\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 88000\n",
            "  training_iteration: 22\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 92000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_agent_steps_trained: 92000\n",
            "    num_env_steps_sampled: 92000\n",
            "    num_env_steps_trained: 92000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-17\n",
            "  done: false\n",
            "  episode_len_mean: 177.74\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 177.74\n",
            "  episode_reward_min: 62.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 969\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.629394531250001e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5701733643649727\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.45720962576808466\n",
            "          kl: 0.003141430308865971\n",
            "          policy_loss: -0.011196844439993622\n",
            "          total_loss: 9.61822319235853\n",
            "          vf_explained_var: -0.0067693726990812565\n",
            "          vf_loss: 9.62942002921976\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_agent_steps_trained: 92000\n",
            "    num_env_steps_sampled: 92000\n",
            "    num_env_steps_trained: 92000\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 92000\n",
            "  num_agent_steps_trained: 92000\n",
            "  num_env_steps_sampled: 92000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 92000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 96.07499999999999\n",
            "    ram_util_percent: 25.2\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10649383298273221\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10201567153540182\n",
            "    mean_inference_ms: 1.672634857174132\n",
            "    mean_raw_obs_processing_ms: 0.5893710775124263\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 177.74\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 177.74\n",
            "    episode_reward_min: 62.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths: [153, 125, 200, 200, 149, 125, 154, 200, 147, 152, 154, 200,\n",
            "        200, 162, 200, 186, 159, 157, 169, 200, 200, 200, 200, 149, 160, 134, 176, 200,\n",
            "        200, 186, 173, 200, 181, 135, 200, 200, 200, 159, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 172, 200, 156, 180, 178, 153, 200, 187, 126, 62, 200, 117,\n",
            "        196, 200, 200, 200, 200, 200, 200, 200, 93, 200, 200, 67, 162, 169, 115, 72,\n",
            "        200, 200, 200, 200, 187, 200, 200, 200, 200, 200, 200, 163, 129, 190, 192, 148,\n",
            "        200, 200, 142, 200, 173, 200, 200, 200]\n",
            "      episode_reward: [153.0, 125.0, 200.0, 200.0, 149.0, 125.0, 154.0, 200.0, 147.0,\n",
            "        152.0, 154.0, 200.0, 200.0, 162.0, 200.0, 186.0, 159.0, 157.0, 169.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 149.0, 160.0, 134.0, 176.0, 200.0, 200.0, 186.0, 173.0,\n",
            "        200.0, 181.0, 135.0, 200.0, 200.0, 200.0, 159.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 172.0, 200.0, 156.0, 180.0, 178.0,\n",
            "        153.0, 200.0, 187.0, 126.0, 62.0, 200.0, 117.0, 196.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 67.0, 162.0, 169.0, 115.0, 72.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 187.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        163.0, 129.0, 190.0, 192.0, 148.0, 200.0, 200.0, 142.0, 200.0, 173.0, 200.0,\n",
            "        200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10649383298273221\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10201567153540182\n",
            "      mean_inference_ms: 1.672634857174132\n",
            "      mean_raw_obs_processing_ms: 0.5893710775124263\n",
            "  time_since_restore: 226.53336000442505\n",
            "  time_this_iter_s: 10.942828893661499\n",
            "  time_total_s: 226.53336000442505\n",
            "  timers:\n",
            "    learn_throughput: 848.723\n",
            "    learn_time_ms: 4712.964\n",
            "    load_throughput: 7185103.212\n",
            "    load_time_ms: 0.557\n",
            "    synch_weights_time_ms: 3.051\n",
            "    training_iteration_time_ms: 9587.867\n",
            "  timestamp: 1700091197\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 92000\n",
            "  training_iteration: 23\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 96000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_agent_steps_trained: 96000\n",
            "    num_env_steps_sampled: 96000\n",
            "    num_env_steps_trained: 96000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-25\n",
            "  done: false\n",
            "  episode_len_mean: 183.15\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 183.15\n",
            "  episode_reward_min: 62.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 990\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.814697265625001e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5919857808338699\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.4572402737794384\n",
            "          kl: 0.0011549935261950578\n",
            "          policy_loss: -0.0092199127640455\n",
            "          total_loss: 9.658959709700717\n",
            "          vf_explained_var: 0.002009341665493545\n",
            "          vf_loss: 9.66817961764592\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_agent_steps_trained: 96000\n",
            "    num_env_steps_sampled: 96000\n",
            "    num_env_steps_trained: 96000\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 96000\n",
            "  num_agent_steps_trained: 96000\n",
            "  num_env_steps_sampled: 96000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 96000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 78.5909090909091\n",
            "    ram_util_percent: 25.2\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10627611441963858\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10161058020473397\n",
            "    mean_inference_ms: 1.6667836967868035\n",
            "    mean_raw_obs_processing_ms: 0.587200889543496\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 183.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 183.15\n",
            "    episode_reward_min: 62.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 149, 160, 134, 176, 200, 200, 186, 173, 200, 181,\n",
            "        135, 200, 200, 200, 159, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 172,\n",
            "        200, 156, 180, 178, 153, 200, 187, 126, 62, 200, 117, 196, 200, 200, 200, 200,\n",
            "        200, 200, 200, 93, 200, 200, 67, 162, 169, 115, 72, 200, 200, 200, 200, 187,\n",
            "        200, 200, 200, 200, 200, 200, 163, 129, 190, 192, 148, 200, 200, 142, 200, 173,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 143, 200, 200, 200, 200, 200, 190, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200]\n",
            "      episode_reward: [200.0, 200.0, 149.0, 160.0, 134.0, 176.0, 200.0, 200.0, 186.0,\n",
            "        173.0, 200.0, 181.0, 135.0, 200.0, 200.0, 200.0, 159.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 172.0, 200.0, 156.0, 180.0,\n",
            "        178.0, 153.0, 200.0, 187.0, 126.0, 62.0, 200.0, 117.0, 196.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 67.0, 162.0, 169.0, 115.0,\n",
            "        72.0, 200.0, 200.0, 200.0, 200.0, 187.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 163.0, 129.0, 190.0, 192.0, 148.0, 200.0, 200.0, 142.0, 200.0, 173.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10627611441963858\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10161058020473397\n",
            "      mean_inference_ms: 1.6667836967868035\n",
            "      mean_raw_obs_processing_ms: 0.587200889543496\n",
            "  time_since_restore: 234.6797354221344\n",
            "  time_this_iter_s: 8.14637541770935\n",
            "  time_total_s: 234.6797354221344\n",
            "  timers:\n",
            "    learn_throughput: 847.007\n",
            "    learn_time_ms: 4722.511\n",
            "    load_throughput: 7032113.337\n",
            "    load_time_ms: 0.569\n",
            "    synch_weights_time_ms: 2.951\n",
            "    training_iteration_time_ms: 9391.594\n",
            "  timestamp: 1700091205\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 96000\n",
            "  training_iteration: 24\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 100000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_agent_steps_trained: 100000\n",
            "    num_env_steps_sampled: 100000\n",
            "    num_env_steps_trained: 100000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-37\n",
            "  done: false\n",
            "  episode_len_mean: 185.61\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.61\n",
            "  episode_reward_min: 62.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1011\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.9073486328125004e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5710928672744382\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5330228557669988\n",
            "          kl: 0.0017794196782803696\n",
            "          policy_loss: -0.011168486156290578\n",
            "          total_loss: 9.62254836892569\n",
            "          vf_explained_var: -0.02416571186434838\n",
            "          vf_loss: 9.633716883710635\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_agent_steps_trained: 100000\n",
            "    num_env_steps_sampled: 100000\n",
            "    num_env_steps_trained: 100000\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 100000\n",
            "  num_agent_steps_trained: 100000\n",
            "  num_env_steps_sampled: 100000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 100000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 91.625\n",
            "    ram_util_percent: 25.225\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10633469213749036\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10146545558429709\n",
            "    mean_inference_ms: 1.6657859344190822\n",
            "    mean_raw_obs_processing_ms: 0.5865336668614353\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 185.61\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 185.61\n",
            "    episode_reward_min: 62.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 200, 200, 200, 172, 200, 156, 180, 178, 153,\n",
            "        200, 187, 126, 62, 200, 117, 196, 200, 200, 200, 200, 200, 200, 200, 93, 200,\n",
            "        200, 67, 162, 169, 115, 72, 200, 200, 200, 200, 187, 200, 200, 200, 200, 200,\n",
            "        200, 163, 129, 190, 192, 148, 200, 200, 142, 200, 173, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 143, 200, 200, 200, 200, 200, 190, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 155, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 144]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 172.0, 200.0, 156.0,\n",
            "        180.0, 178.0, 153.0, 200.0, 187.0, 126.0, 62.0, 200.0, 117.0, 196.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 67.0, 162.0, 169.0,\n",
            "        115.0, 72.0, 200.0, 200.0, 200.0, 200.0, 187.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 163.0, 129.0, 190.0, 192.0, 148.0, 200.0, 200.0, 142.0, 200.0,\n",
            "        173.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 143.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 155.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 144.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10633469213749036\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10146545558429709\n",
            "      mean_inference_ms: 1.6657859344190822\n",
            "      mean_raw_obs_processing_ms: 0.5865336668614353\n",
            "  time_since_restore: 245.8038194179535\n",
            "  time_this_iter_s: 11.124083995819092\n",
            "  time_total_s: 245.8038194179535\n",
            "  timers:\n",
            "    learn_throughput: 862.0\n",
            "    learn_time_ms: 4640.37\n",
            "    load_throughput: 5840225.572\n",
            "    load_time_ms: 0.685\n",
            "    synch_weights_time_ms: 2.504\n",
            "    training_iteration_time_ms: 9516.53\n",
            "  timestamp: 1700091217\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 100000\n",
            "  training_iteration: 25\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 104000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_agent_steps_trained: 104000\n",
            "    num_env_steps_sampled: 104000\n",
            "    num_env_steps_trained: 104000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-45\n",
            "  done: false\n",
            "  episode_len_mean: 187.93\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 187.93\n",
            "  episode_reward_min: 67.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1032\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.536743164062502e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5765930559045525\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6259124158290765\n",
            "          kl: 0.0012625845264926038\n",
            "          policy_loss: -0.007373453998157094\n",
            "          total_loss: 9.592625651821013\n",
            "          vf_explained_var: 0.010056112594501947\n",
            "          vf_loss: 9.59999904735114\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_agent_steps_trained: 104000\n",
            "    num_env_steps_sampled: 104000\n",
            "    num_env_steps_trained: 104000\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 104000\n",
            "  num_agent_steps_trained: 104000\n",
            "  num_env_steps_sampled: 104000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 104000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 76.59166666666665\n",
            "    ram_util_percent: 25.2\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10629138518432119\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10126981699816998\n",
            "    mean_inference_ms: 1.6638740461383972\n",
            "    mean_raw_obs_processing_ms: 0.5857317581758188\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 187.93\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 187.93\n",
            "    episode_reward_min: 67.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 200, 200, 93, 200, 200, 67, 162, 169, 115, 72,\n",
            "        200, 200, 200, 200, 187, 200, 200, 200, 200, 200, 200, 163, 129, 190, 192, 148,\n",
            "        200, 200, 142, 200, 173, 200, 200, 200, 200, 200, 200, 200, 200, 143, 200, 200,\n",
            "        200, 200, 200, 190, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 155, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 144, 200, 200, 200, 200, 196, 200, 200, 200, 200, 200, 200, 200, 200, 91,\n",
            "        200, 180, 200, 92, 200, 200, 200]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 67.0,\n",
            "        162.0, 169.0, 115.0, 72.0, 200.0, 200.0, 200.0, 200.0, 187.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 163.0, 129.0, 190.0, 192.0, 148.0, 200.0, 200.0,\n",
            "        142.0, 200.0, 173.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        143.0, 200.0, 200.0, 200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 155.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 144.0, 200.0, 200.0, 200.0, 200.0, 196.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 91.0, 200.0, 180.0, 200.0, 92.0, 200.0,\n",
            "        200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10629138518432119\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10126981699816998\n",
            "      mean_inference_ms: 1.6638740461383972\n",
            "      mean_raw_obs_processing_ms: 0.5857317581758188\n",
            "  time_since_restore: 253.86253213882446\n",
            "  time_this_iter_s: 8.058712720870972\n",
            "  time_total_s: 253.86253213882446\n",
            "  timers:\n",
            "    learn_throughput: 863.641\n",
            "    learn_time_ms: 4631.556\n",
            "    load_throughput: 5804662.492\n",
            "    load_time_ms: 0.689\n",
            "    synch_weights_time_ms: 2.808\n",
            "    training_iteration_time_ms: 9369.213\n",
            "  timestamp: 1700091225\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 104000\n",
            "  training_iteration: 26\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 108000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_agent_steps_trained: 108000\n",
            "    num_env_steps_sampled: 108000\n",
            "    num_env_steps_trained: 108000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-33-56\n",
            "  done: false\n",
            "  episode_len_mean: 191.8\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.8\n",
            "  episode_reward_min: 91.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1053\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.768371582031251e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5701812139762346\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5418923403026276\n",
            "          kl: 0.0006041624502289594\n",
            "          policy_loss: -0.013191168100362824\n",
            "          total_loss: 9.658048465687742\n",
            "          vf_explained_var: -0.03842911528002831\n",
            "          vf_loss: 9.671239616024879\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_agent_steps_trained: 108000\n",
            "    num_env_steps_sampled: 108000\n",
            "    num_env_steps_trained: 108000\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 108000\n",
            "  num_agent_steps_trained: 108000\n",
            "  num_env_steps_sampled: 108000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 108000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 87.925\n",
            "    ram_util_percent: 25.2\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10670727303025687\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10166581544370924\n",
            "    mean_inference_ms: 1.6676424442903237\n",
            "    mean_raw_obs_processing_ms: 0.5868230834560664\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 191.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 191.8\n",
            "    episode_reward_min: 91.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 163, 129, 190, 192, 148, 200, 200, 142, 200,\n",
            "        173, 200, 200, 200, 200, 200, 200, 200, 200, 143, 200, 200, 200, 200, 200, 190,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 155, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 144, 200, 200,\n",
            "        200, 200, 196, 200, 200, 200, 200, 200, 200, 200, 200, 91, 200, 180, 200, 92,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 127, 200, 197,\n",
            "        200, 138, 198, 200, 200, 200, 192, 200]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 163.0, 129.0, 190.0, 192.0, 148.0, 200.0,\n",
            "        200.0, 142.0, 200.0, 173.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 143.0, 200.0, 200.0, 200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 155.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 144.0, 200.0, 200.0, 200.0, 200.0, 196.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 91.0, 200.0, 180.0, 200.0,\n",
            "        92.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 127.0, 200.0, 197.0, 200.0, 138.0, 198.0, 200.0, 200.0,\n",
            "        200.0, 192.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10670727303025687\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10166581544370924\n",
            "      mean_inference_ms: 1.6676424442903237\n",
            "      mean_raw_obs_processing_ms: 0.5868230834560664\n",
            "  time_since_restore: 265.1806163787842\n",
            "  time_this_iter_s: 11.318084239959717\n",
            "  time_total_s: 265.1806163787842\n",
            "  timers:\n",
            "    learn_throughput: 880.564\n",
            "    learn_time_ms: 4542.541\n",
            "    load_throughput: 5753306.128\n",
            "    load_time_ms: 0.695\n",
            "    synch_weights_time_ms: 2.738\n",
            "    training_iteration_time_ms: 9531.259\n",
            "  timestamp: 1700091236\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 108000\n",
            "  training_iteration: 27\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 112000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_agent_steps_trained: 112000\n",
            "    num_env_steps_sampled: 112000\n",
            "    num_env_steps_trained: 112000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-34-05\n",
            "  done: false\n",
            "  episode_len_mean: 193.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 193.36\n",
            "  episode_reward_min: 91.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1073\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.3841857910156255e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5791327810415657\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6190382346230489\n",
            "          kl: 0.0009506127439744583\n",
            "          policy_loss: -0.0074973329621297055\n",
            "          total_loss: 9.574580979090864\n",
            "          vf_explained_var: -0.038241451786410426\n",
            "          vf_loss: 9.582078339976649\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_agent_steps_trained: 112000\n",
            "    num_env_steps_sampled: 112000\n",
            "    num_env_steps_trained: 112000\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 112000\n",
            "  num_agent_steps_trained: 112000\n",
            "  num_env_steps_sampled: 112000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 112000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 84.11666666666666\n",
            "    ram_util_percent: 25.25\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10684498350004837\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10187748388981176\n",
            "    mean_inference_ms: 1.6682598057510378\n",
            "    mean_raw_obs_processing_ms: 0.5868287888543807\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 193.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 193.36\n",
            "    episode_reward_min: 91.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 143, 200, 200, 200, 200, 200, 190, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 155, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 144, 200, 200, 200, 200, 196, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 91, 200, 180, 200, 92, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 127, 200, 197, 200, 138, 198, 200,\n",
            "        200, 200, 192, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 93, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200]\n",
            "      episode_reward: [200.0, 143.0, 200.0, 200.0, 200.0, 200.0, 200.0, 190.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 155.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 144.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        196.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 91.0, 200.0,\n",
            "        180.0, 200.0, 92.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 127.0, 200.0, 197.0, 200.0, 138.0, 198.0,\n",
            "        200.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10684498350004837\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10187748388981176\n",
            "      mean_inference_ms: 1.6682598057510378\n",
            "      mean_raw_obs_processing_ms: 0.5868287888543807\n",
            "  time_since_restore: 273.6752083301544\n",
            "  time_this_iter_s: 8.49459195137024\n",
            "  time_total_s: 273.6752083301544\n",
            "  timers:\n",
            "    learn_throughput: 869.051\n",
            "    learn_time_ms: 4602.724\n",
            "    load_throughput: 5740117.695\n",
            "    load_time_ms: 0.697\n",
            "    synch_weights_time_ms: 2.742\n",
            "    training_iteration_time_ms: 9508.016\n",
            "  timestamp: 1700091245\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 112000\n",
            "  training_iteration: 28\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 116000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_agent_steps_trained: 116000\n",
            "    num_env_steps_sampled: 116000\n",
            "    num_env_steps_trained: 116000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-34-16\n",
            "  done: false\n",
            "  episode_len_mean: 192.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.36\n",
            "  episode_reward_min: 63.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1094\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1920928955078127e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5757332672995906\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.585030898928482\n",
            "          kl: 0.0012574482534272742\n",
            "          policy_loss: -0.00841933690632383\n",
            "          total_loss: 9.558297486459056\n",
            "          vf_explained_var: -0.045479560218831545\n",
            "          vf_loss: 9.566716831986621\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_agent_steps_trained: 116000\n",
            "    num_env_steps_sampled: 116000\n",
            "    num_env_steps_trained: 116000\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 116000\n",
            "  num_agent_steps_trained: 116000\n",
            "  num_env_steps_sampled: 116000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 116000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 83.75625\n",
            "    ram_util_percent: 25.25\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1072566880850109\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10239401867269118\n",
            "    mean_inference_ms: 1.6752686531036218\n",
            "    mean_raw_obs_processing_ms: 0.5886321900408056\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 192.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 192.36\n",
            "    episode_reward_min: 63.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths: [155, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 144, 200, 200, 200, 200, 196, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 91, 200, 180, 200, 92, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 127, 200, 197, 200, 138, 198, 200, 200, 200, 192, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 93, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 170, 200, 200, 200, 63,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200]\n",
            "      episode_reward: [155.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 144.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 91.0,\n",
            "        200.0, 180.0, 200.0, 92.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 127.0, 200.0, 197.0, 200.0, 138.0,\n",
            "        198.0, 200.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 170.0, 200.0, 200.0, 200.0, 63.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1072566880850109\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10239401867269118\n",
            "      mean_inference_ms: 1.6752686531036218\n",
            "      mean_raw_obs_processing_ms: 0.5886321900408056\n",
            "  time_since_restore: 284.72132110595703\n",
            "  time_this_iter_s: 11.046112775802612\n",
            "  time_total_s: 284.72132110595703\n",
            "  timers:\n",
            "    learn_throughput: 894.458\n",
            "    learn_time_ms: 4471.98\n",
            "    load_throughput: 5951477.829\n",
            "    load_time_ms: 0.672\n",
            "    synch_weights_time_ms: 2.477\n",
            "    training_iteration_time_ms: 9626.807\n",
            "  timestamp: 1700091256\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 116000\n",
            "  training_iteration: 29\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 120000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_agent_steps_trained: 120000\n",
            "    num_env_steps_sampled: 120000\n",
            "    num_env_steps_trained: 120000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-34-25\n",
            "  done: false\n",
            "  episode_len_mean: 192.31\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.31\n",
            "  episode_reward_min: 63.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1114\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.960464477539064e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5740709975201597\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.5389655165534507\n",
            "          kl: 0.0006753924436297287\n",
            "          policy_loss: -0.006523199103051616\n",
            "          total_loss: 9.579215470180717\n",
            "          vf_explained_var: -0.03955263674900096\n",
            "          vf_loss: 9.585738674286873\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_agent_steps_trained: 120000\n",
            "    num_env_steps_sampled: 120000\n",
            "    num_env_steps_trained: 120000\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 120000\n",
            "  num_agent_steps_trained: 120000\n",
            "  num_env_steps_sampled: 120000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 120000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 90.45384615384616\n",
            "    ram_util_percent: 25.292307692307695\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10735426883639664\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1026385268966706\n",
            "    mean_inference_ms: 1.6773439978699685\n",
            "    mean_raw_obs_processing_ms: 0.5889269041500271\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 192.31\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 192.31\n",
            "    episode_reward_min: 63.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 196, 200, 200, 200, 200, 200, 200, 200, 200, 91, 200, 180,\n",
            "        200, 92, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 127,\n",
            "        200, 197, 200, 138, 198, 200, 200, 200, 192, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 93, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 170, 200, 200, 200, 63, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 101, 200, 200, 200]\n",
            "      episode_reward: [200.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 91.0, 200.0, 180.0, 200.0, 92.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 127.0, 200.0, 197.0, 200.0,\n",
            "        138.0, 198.0, 200.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 170.0, 200.0, 200.0, 200.0, 63.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 193.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 101.0, 200.0,\n",
            "        200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10735426883639664\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1026385268966706\n",
            "      mean_inference_ms: 1.6773439978699685\n",
            "      mean_raw_obs_processing_ms: 0.5889269041500271\n",
            "  time_since_restore: 293.90175557136536\n",
            "  time_this_iter_s: 9.180434465408325\n",
            "  time_total_s: 293.90175557136536\n",
            "  timers:\n",
            "    learn_throughput: 870.115\n",
            "    learn_time_ms: 4597.092\n",
            "    load_throughput: 5993361.197\n",
            "    load_time_ms: 0.667\n",
            "    synch_weights_time_ms: 3.197\n",
            "    training_iteration_time_ms: 9702.423\n",
            "  timestamp: 1700091265\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 120000\n",
            "  training_iteration: 30\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 124000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_agent_steps_trained: 124000\n",
            "    num_env_steps_sampled: 124000\n",
            "    num_env_steps_trained: 124000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-34-36\n",
            "  done: false\n",
            "  episode_len_mean: 194.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 194.72\n",
            "  episode_reward_min: 63.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1134\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.980232238769532e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5749481001207906\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.6783574536683098\n",
            "          kl: 0.00024732203829189414\n",
            "          policy_loss: -0.005757057434448632\n",
            "          total_loss: 9.588418872382052\n",
            "          vf_explained_var: -0.03832805592526672\n",
            "          vf_loss: 9.59417599503712\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_agent_steps_trained: 124000\n",
            "    num_env_steps_sampled: 124000\n",
            "    num_env_steps_trained: 124000\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 124000\n",
            "  num_agent_steps_trained: 124000\n",
            "  num_env_steps_sampled: 124000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 124000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 82.30666666666667\n",
            "    ram_util_percent: 25.29333333333334\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10770269026483575\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1031395973490644\n",
            "    mean_inference_ms: 1.6839289347084132\n",
            "    mean_raw_obs_processing_ms: 0.5905633062772772\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 194.72\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 194.72\n",
            "    episode_reward_min: 63.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 200, 200, 200, 200, 200, 127, 200, 197, 200,\n",
            "        138, 198, 200, 200, 200, 192, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 93, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 170, 200, 200, 200, 63, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        101, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 127.0,\n",
            "        200.0, 197.0, 200.0, 138.0, 198.0, 200.0, 200.0, 200.0, 192.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 93.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 170.0, 200.0, 200.0, 200.0, 63.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        193.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 101.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10770269026483575\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1031395973490644\n",
            "      mean_inference_ms: 1.6839289347084132\n",
            "      mean_raw_obs_processing_ms: 0.5905633062772772\n",
            "  time_since_restore: 304.408358335495\n",
            "  time_this_iter_s: 10.506602764129639\n",
            "  time_total_s: 304.408358335495\n",
            "  timers:\n",
            "    learn_throughput: 907.105\n",
            "    learn_time_ms: 4409.635\n",
            "    load_throughput: 6238970.659\n",
            "    load_time_ms: 0.641\n",
            "    synch_weights_time_ms: 3.078\n",
            "    training_iteration_time_ms: 9672.557\n",
            "  timestamp: 1700091276\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 124000\n",
            "  training_iteration: 31\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n",
            "Result for PPO_CartPole-v0_c676f_00000:\n",
            "  agent_timesteps_total: 128000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_agent_steps_trained: 128000\n",
            "    num_env_steps_sampled: 128000\n",
            "    num_env_steps_trained: 128000\n",
            "  custom_metrics: {}\n",
            "  date: 2023-11-15_23-34-45\n",
            "  done: true\n",
            "  episode_len_mean: 196.2\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 196.2\n",
            "  episode_reward_min: 63.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1154\n",
            "  experiment_id: fd36aae4251b4b6883d25add6fce1680\n",
            "  hostname: f9a267b53a56\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.490116119384766e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5818614120124489\n",
            "          entropy_coeff: 0.0\n",
            "          grad_gnorm: 0.8066340504654793\n",
            "          kl: 0.0024858186761997753\n",
            "          policy_loss: -0.008021097492066122\n",
            "          total_loss: 9.578718129024711\n",
            "          vf_explained_var: -0.043636822892773534\n",
            "          vf_loss: 9.586739209903184\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_agent_steps_trained: 128000\n",
            "    num_env_steps_sampled: 128000\n",
            "    num_env_steps_trained: 128000\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.12\n",
            "  num_agent_steps_sampled: 128000\n",
            "  num_agent_steps_trained: 128000\n",
            "  num_env_steps_sampled: 128000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 128000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_faulty_episodes: 0\n",
            "  num_healthy_workers: 2\n",
            "  num_recreated_workers: 0\n",
            "  num_steps_trained_this_iter: 4000\n",
            "  perf:\n",
            "    cpu_util_percent: 94.40769230769229\n",
            "    ram_util_percent: 25.27692307692308\n",
            "  pid: 1698\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10760120741629536\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10311656000503602\n",
            "    mean_inference_ms: 1.6848933812173066\n",
            "    mean_raw_obs_processing_ms: 0.590338514736739\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 196.2\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 196.2\n",
            "    episode_reward_min: 63.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths: [200, 200, 200, 200, 200, 200, 200, 200, 200, 93, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 170, 200,\n",
            "        200, 200, 63, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 193,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 101, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
            "        200, 200, 200, 200, 200, 200, 200]\n",
            "      episode_reward: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        93.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 170.0, 200.0, 200.0, 200.0,\n",
            "        63.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 193.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 101.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0,\n",
            "        200.0, 200.0, 200.0]\n",
            "    num_faulty_episodes: 0\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10760120741629536\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10311656000503602\n",
            "      mean_inference_ms: 1.6848933812173066\n",
            "      mean_raw_obs_processing_ms: 0.590338514736739\n",
            "  time_since_restore: 313.7222650051117\n",
            "  time_this_iter_s: 9.3139066696167\n",
            "  time_total_s: 313.7222650051117\n",
            "  timers:\n",
            "    learn_throughput: 879.825\n",
            "    learn_time_ms: 4546.36\n",
            "    load_throughput: 6251757.341\n",
            "    load_time_ms: 0.64\n",
            "    synch_weights_time_ms: 3.536\n",
            "    training_iteration_time_ms: 9805.234\n",
            "  timestamp: 1700091285\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 128000\n",
            "  training_iteration: 32\n",
            "  trial_id: c676f_00000\n",
            "  warmup_time: 12.533946514129639\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-15 23:34:46,000\tINFO tune.py:758 -- Total run time: 337.23 seconds (336.49 seconds for the tuning loop).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
        "checkpoint = analysis.get_best_checkpoint(\n",
        "    trial,\n",
        "    \"training_iteration\",\n",
        "    \"max\",\n",
        ")\n",
        "trainer = PPOTrainer(config=config)\n",
        "trainer.restore(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF6ixZmsFu8z",
        "outputId": "11f5fcff-8f10-48de-aae1-9be7ded61d40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "2023-11-15 23:34:47,087\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            "2023-11-15 23:34:47,093\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
            "  warnings.warn(\n",
            "2023-11-15 23:35:07,518\tINFO trainable.py:160 -- Trainable.setup took 20.432 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-11-15 23:35:07,523\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            "2023-11-15 23:35:07,533\tINFO trainable.py:668 -- Restored on 172.28.0.12 from checkpoint: /root/ray_results/PPO/PPO_CartPole-v0_c676f_00000_0_2023-11-15_23-29-09/checkpoint_000032\n",
            "2023-11-15 23:35:07,538\tINFO trainable.py:677 -- Current state after restoring: {'_iteration': 32, '_timesteps_total': None, '_time_total': 313.7222650051117, '_episodes_total': 1154}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "after_training = \"after_training.mp4\"\n",
        "after_video = VideoRecorder(env, after_training)\n",
        "observation = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  env.render()\n",
        "  after_video.capture_frame()\n",
        "  action = trainer.compute_action(observation)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "after_video.close()\n",
        "env.close()\n",
        "html = render_mp4(after_training)\n",
        "HTML(html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "g-qWu8gLJnOD",
        "outputId": "a09aa2fc-3295-436a-9469-178c4f953a99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "2023-11-15 23:35:07,815\tWARNING deprecation.py:47 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyZmuRIUKFX8",
        "outputId": "5d76bb0c-5d97-4a62-8962-94fd0a57d9b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}